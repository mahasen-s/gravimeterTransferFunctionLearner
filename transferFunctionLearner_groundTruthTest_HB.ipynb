{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math\n",
    "import h5py\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float64\n",
    "DTYPE_np = np.float64\n",
    "log_dir = '/home/mahasen/tf_logs/transferFuncLearner' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        #with tf.name_scope('biases'):\n",
    "        #    biases = get_biases([output_dim],DTYPE)\n",
    "        #    variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        #logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        logits = tf.matmul(input_tensor, weights,name='logits') # don't need biases if we're using batch norms\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processed campaign4a\n",
    "Matlab used to assign subsequences in `accelerometer` to each entry in `ai_kup` and `ai_kdown`.\n",
    "Subsequence length is 195. ~100000 entries for each of `ai_kup` and `ai_kdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that will generate indices for batches\n",
    "# accounting for the possibility that the size of the dataset\n",
    "# will not be an even multiple of the batch size\n",
    "def generate_batches(N,batch_size=32):\n",
    "    # N is the number of elements in the dataset\n",
    "    if N<batch_size:\n",
    "        raise ValueError('batch_size must be smaller than N')\n",
    "    perm = np.random.permutation(N);\n",
    "    if np.mod(N,batch_size)!=0:\n",
    "        # We need to append to perm so that is is an even multiple of batch_size\n",
    "        perm2= np.random.permutation(N);\n",
    "        perm = np.concatenate((perm,perm2[0:batch_size-np.mod(N,batch_size)]))\n",
    "    \n",
    "    n_batches = np.int(len(perm)/batch_size)\n",
    "    batches = perm.reshape(batch_size,n_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def get_xy_by_inds(x,y,inds):\n",
    "    # Get values from 2Dx, 1Dy arrays\n",
    "    x0 = x[inds,:]\n",
    "    y0 = np.array(list(map(y.__getitem__,inds))) # because Python treats 1D arrays differently from ND arrays\n",
    "    y0 = y0.reshape([len(y0),1])\n",
    "    return x0,y0\n",
    "    \n",
    "def generate_testtrain(x,y,test_fraction,batch_size=32):\n",
    "    # Assumes x.shape = [n_samples,input_size], y.shape = [input_size,]\n",
    "    if test_fraction<0 or test_fraction>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    \n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*test_fraction))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "    train_size = np.int(n_samples-test_size)\n",
    "    \n",
    "    # Permute x,y\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    permi= np.argsort(perm)\n",
    "    x0,y0= get_xy_by_inds(x,y,permi)\n",
    "                    \n",
    "    train_x = x0[:train_size,:]\n",
    "    train_y = y0[:train_size,:]\n",
    "    \n",
    "    test_x  = x0[train_size:,:]\n",
    "    test_y  = y0[train_size:,:]\n",
    "    \n",
    "    batches = generate_batches(train_size,batch_size)\n",
    "    \n",
    "    return test_x,test_y,train_x,train_y,batches\n",
    "\n",
    "def generate_groundtruthtest(x,y,ground_truth_frac,ground_truth_periods):\n",
    "    # Separate input data into (x,y) pairs for training and (x,y) pairs for ground truth testing\n",
    "    if ground_truth_frac<0 or ground_truth_frac>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*ground_truth_frac/ground_truth_periods))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "        \n",
    "    n_samples_per_period = np.int(np.round(n_samples/ground_truth_periods))\n",
    "    n_test_samples_per_period = np.int(np.round(n_samples_per_period*ground_truth_frac))\n",
    "    n_train_samples_per_period = np.int(np.round(n_samples_per_period - n_test_samples_per_period))\n",
    "                                        \n",
    "    train_inds = np.array([])\n",
    "    test_inds = np.array([])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(ground_truth_periods):\n",
    "        if i!=(ground_truth_periods-1):\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,counter+n_train_samples_per_period))])\n",
    "            counter += n_train_samples_per_period\n",
    "        else:\n",
    "            # Assume n_samples>>ground_truth_periods\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,n_samples))])\n",
    "            counter += (n_samples-counter)\n",
    "    test_inds = [np.int(i) for i in test_inds]\n",
    "    train_inds = [np.int(i) for i in train_inds]\n",
    "    \n",
    "    train_x,train_y   = get_xy_by_inds(x,y,train_inds)\n",
    "    test_x,test_y     = get_xy_by_inds(x,y,test_inds)    \n",
    "    \n",
    "    return test_inds,test_x,test_y,train_inds,train_x,train_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5362c81743c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_inds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerate_groundtruthtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3849bfcfcdf>\u001b[0m in \u001b[0;36mgenerate_groundtruthtest\u001b[0;34m(x, y, ground_truth_frac, ground_truth_periods)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3849bfcfcdf>\u001b[0m in \u001b[0;36mget_xy_by_inds\u001b[0;34m(x, y, inds)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Get values from 2Dx, 1Dy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# because Python treats 1D arrays differently from ND arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "x = np.array(list(range(0,100000)))\n",
    "y = np.zeros(100000)\n",
    "test_inds,train_inds= generate_groundtruthtest(x,y,0.2,7)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_sign = np.zeros(100000)\n",
    "for i in test_inds:\n",
    "    test_sign[i] = 1\n",
    "    \n",
    "plt.plot(x,test_sign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_deep_learner_arbshape(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is a LIST of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    num_layers = len(h)\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h[i],h[i+1],'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h[-1],n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  batch_size=32, epochs =5, test_fraction=0.1, test_interval=250, initial_learning_rate = 0.02,\n",
    "                  epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 10,\n",
    "                  sgd_switch_epoch = 3,prob_keep = 0.8,\n",
    "                  ground_truth_frac=0.2, ground_truth_periods = 7,\n",
    "                  fname_model_out='/tmp/model.ckpt',\n",
    "                  fname_data_out='/tmp/model_data.pkl'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 1}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        # Separate data into ground truth test and train data\n",
    "        ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(x_data,y_data,ground_truth_frac,ground_truth_periods)\n",
    "        \n",
    "        # Generate test data initially for convenience\n",
    "        test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "        n_samples = len(y_data)\n",
    "        max_steps = batches.shape[1]\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            #learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_ind, epoch_learning_rate_interval, epoch_learning_rate_decay, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step      = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "            #train_step = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        ground_truthtest_writer = tf.summary.FileWriter(log_dir + '/ground_truth_test', sess.graph)\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            # At each epoch, regenerate test data, train data, and batches\n",
    "            test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(x_data,y_data,test_fraction,batch_size)\n",
    "            n_batches = batches.shape[1]\n",
    "            n_test = len(test_y)\n",
    "            epoch_init_time = time.time()\n",
    "                \n",
    "            print('Starting Epoch %d, with %d batches' % (j,max_steps))\n",
    "            for i in range(n_batches):\n",
    "                if i % test_interval ==0:\n",
    "                    # Check how our model performs against the epoch test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: ground_truth_test_x, y: ground_truth_test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    # Check how our model performs against the ground truth test data\n",
    "                    [summary, ground_mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    ground_truthtest_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('\\tEpoch %d, Step %04d, MSE: %4.3e, Ground truth MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val,ground_mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = get_xy_by_inds(train_x_full,train_y_full,batches[:,i])\n",
    "                    if (np.mod(j,epoch_learning_rate_interval)<sgd_switch_epoch):\n",
    "                        summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    else:\n",
    "                        summary, _ = sess.run([merged_summaries, train_step_sgd], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            # End of epoch, calculate avg stats\n",
    "            epoch_end_time = time.time()\n",
    "            sess.run(increment_epoch_ind)\n",
    "            avg_mrad = np.sqrt(mse_val)*1000\n",
    "            print('Epoch %d time: %4.3fs, average error = %4.3fmrad' % (j,epoch_end_time-epoch_init_time,avg_mrad))\n",
    "        train_writer.close()\n",
    "        test_writer.close()  \n",
    "        ground_truthtest_writer.close()\n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))\n",
    "        \n",
    "        # Save model\n",
    "        save_path = saver.save(sess, fname_model_out)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        # Save parameters, data\n",
    "        with h5py.File(fname_data_out,'w') as file:\n",
    "            file.create_dataset('/ground_truth_test_inds', data=ground_truth_test_inds)\n",
    "            file.create_dataset('/ground_truth_train_inds', data=ground_truth_train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv_inference(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  fname_model_in='/tmp/model.ckpt'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_in)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: False})\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(acc,phase,fname_model_out='',fname_data_out=''):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    batch_size  = 768\n",
    "    \n",
    "    mccv_steps   = 5000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 1000,sgd_switch_epoch = 750,\n",
    "                            prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 7,fname_model_out=fname_model_out,fname_data_out=fname_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(acc,phase,fname):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    \n",
    "    preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n",
    "                            x_data=acc,y_data=phase,fname_model_in=fname)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_in = './campaign4_proc_1000ms.h5'\n",
    "fname_out = './campaign4_proc_1000ms_preds_up.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for inference\n",
    "fname_infer_in  = './campaign4_proc_1000ms.h5'\n",
    "fname_infer_out = './campaign4_proc_1000ms_preds_up.h5'\n",
    "with h5py.File(fname_infer_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    #dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    infer_ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    infer_ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    infer_ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    #ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    #ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    #ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_ground_truth_1_HB.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Do inference\n",
    "reset_tf()\n",
    "preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4_1000ms_HB.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "#shutil.copy(fname_in,fname_out)\n",
    "ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(ai_kup_acc,ai_kup_phase,0.2,5)\n",
    "with h5py.File(fname_infer_out,'w') as file:\n",
    "    file.create_dataset('/ai_kup/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/preds', data=preds_up.reshape(len(preds_up)), dtype='float64')\n",
    "    file.create_dataset('/ai_kup/ground_truth_test_inds', data=np.array(ground_truth_test_inds).reshape(len(ground_truth_test_inds)), dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1712.07628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0, with 156 batches\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: func_learner/hidden_layer_0/weights/summaries/Max = Max[T=DT_DOUBLE, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](func_learner/hidden_layer_0/weights/weights/read, func_learner/hidden_layer_0/weights/summaries/range_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: func_learner/hidden_layer_4/weights/summaries/stddev/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_349_func_learner/hidden_layer_4/weights/summaries/stddev\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'func_learner/hidden_layer_0/weights/summaries/Max', defined at:\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-dad7ab1fef9d>\", line 1, in <module>\n    train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4_1000ms_HB.ckpt',fname_data_out='./model_ground_truth_campaign4_1000ms_HB.h5')\n  File \"<ipython-input-14-64ea29b26113>\", line 18, in train_model\n    prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 7,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n  File \"<ipython-input-6-ef5ee2cd0490>\", line 35, in train_RBM_complete_mccv\n    y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n  File \"<ipython-input-5-1419f7eee56e>\", line 16, in func_deep_learner_arbshape\n    hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n  File \"<ipython-input-2-2cbbbb5f05cc>\", line 66, in get_layer_complete\n    variable_summaries(weights)\n  File \"<ipython-input-2-2cbbbb5f05cc>\", line 39, in variable_summaries\n    tf.summary.scalar('max',tf.reduce_max(var))\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1601, in reduce_max\n    name=name))\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4360, in _max\n    name=name)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: func_learner/hidden_layer_0/weights/summaries/Max = Max[T=DT_DOUBLE, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](func_learner/hidden_layer_0/weights/weights/read, func_learner/hidden_layer_0/weights/summaries/range_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: func_learner/hidden_layer_4/weights/summaries/stddev/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_349_func_learner/hidden_layer_4/weights/summaries/stddev\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: func_learner/hidden_layer_0/weights/summaries/Max = Max[T=DT_DOUBLE, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](func_learner/hidden_layer_0/weights/weights/read, func_learner/hidden_layer_0/weights/summaries/range_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: func_learner/hidden_layer_4/weights/summaries/stddev/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_349_func_learner/hidden_layer_4/weights/summaries/stddev\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dad7ab1fef9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai_kup_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mai_kup_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_model_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms_HB.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_data_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms_HB.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-64ea29b26113>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(acc, phase, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     16\u001b[0m                             \u001b[0mx_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmccv_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                             \u001b[0mepoch_learning_rate_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_learning_rate_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_switch_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                             prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 7,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ef5ee2cd0490>\u001b[0m in \u001b[0;36mtrain_RBM_complete_mccv\u001b[0;34m(input_size, output_size, hidden_sizes, x_data, y_data, batch_size, epochs, test_fraction, test_interval, initial_learning_rate, epoch_learning_rate_decay, epoch_learning_rate_interval, sgd_switch_epoch, prob_keep, ground_truth_frac, ground_truth_periods, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_interval\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;31m# Check how our model performs against the epoch test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mground_truth_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mground_truth_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: func_learner/hidden_layer_0/weights/summaries/Max = Max[T=DT_DOUBLE, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](func_learner/hidden_layer_0/weights/weights/read, func_learner/hidden_layer_0/weights/summaries/range_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: func_learner/hidden_layer_4/weights/summaries/stddev/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_349_func_learner/hidden_layer_4/weights/summaries/stddev\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'func_learner/hidden_layer_0/weights/summaries/Max', defined at:\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-dad7ab1fef9d>\", line 1, in <module>\n    train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4_1000ms_HB.ckpt',fname_data_out='./model_ground_truth_campaign4_1000ms_HB.h5')\n  File \"<ipython-input-14-64ea29b26113>\", line 18, in train_model\n    prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 7,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n  File \"<ipython-input-6-ef5ee2cd0490>\", line 35, in train_RBM_complete_mccv\n    y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n  File \"<ipython-input-5-1419f7eee56e>\", line 16, in func_deep_learner_arbshape\n    hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n  File \"<ipython-input-2-2cbbbb5f05cc>\", line 66, in get_layer_complete\n    variable_summaries(weights)\n  File \"<ipython-input-2-2cbbbb5f05cc>\", line 39, in variable_summaries\n    tf.summary.scalar('max',tf.reduce_max(var))\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\n    return func(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1601, in reduce_max\n    name=name))\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 4360, in _max\n    name=name)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[] and type double on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: func_learner/hidden_layer_0/weights/summaries/Max = Max[T=DT_DOUBLE, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](func_learner/hidden_layer_0/weights/weights/read, func_learner/hidden_layer_0/weights/summaries/range_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: func_learner/hidden_layer_4/weights/summaries/stddev/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_349_func_learner/hidden_layer_4/weights/summaries/stddev\", tensor_type=DT_DOUBLE, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4_1000ms_HB.ckpt',fname_data_out='./model_ground_truth_campaign4_1000ms_HB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
