{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahasen/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math\n",
    "import h5py\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float64\n",
    "DTYPE_np = np.float64\n",
    "log_dir = '/mnt/dataDrive3/mahasen/tmp/funclearn' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        #with tf.name_scope('biases'):\n",
    "        #    biases = get_biases([output_dim],DTYPE)\n",
    "        #    variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        #logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        logits = tf.matmul(input_tensor, weights,name='logits') # don't need biases if we're using batch norms\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processed campaign4a\n",
    "Matlab used to assign subsequences in `accelerometer` to each entry in `ai_kup` and `ai_kdown`.\n",
    "Subsequence length is 195. ~100000 entries for each of `ai_kup` and `ai_kdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that will generate indices for batches\n",
    "# accounting for the possibility that the size of the dataset\n",
    "# will not be an even multiple of the batch size\n",
    "def generate_batches(N,batch_size=32):\n",
    "    # N is the number of elements in the dataset\n",
    "    if N<batch_size:\n",
    "        raise ValueError('batch_size must be smaller than N')\n",
    "    perm = np.random.permutation(N);\n",
    "    if np.mod(N,batch_size)!=0:\n",
    "        # We need to append to perm so that is is an even multiple of batch_size\n",
    "        perm2= np.random.permutation(N);\n",
    "        perm = np.concatenate((perm,perm2[0:batch_size-np.mod(N,batch_size)]))\n",
    "    \n",
    "    n_batches = np.int(len(perm)/batch_size)\n",
    "    batches = perm.reshape(batch_size,n_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def get_xy_by_inds(x,y,inds):\n",
    "    # Get values from 2Dx, 1Dy arrays\n",
    "    x0 = x[inds,:]\n",
    "    y0 = np.array(list(map(y.__getitem__,inds))) # because Python treats 1D arrays differently from ND arrays\n",
    "    y0 = y0.reshape([len(y0),1])\n",
    "    return x0,y0\n",
    "    \n",
    "def generate_testtrain(x,y,test_fraction,batch_size=32):\n",
    "    # Assumes x.shape = [n_samples,input_size], y.shape = [input_size,]\n",
    "    if test_fraction<0 or test_fraction>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    \n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*test_fraction))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "    train_size = np.int(n_samples-test_size)\n",
    "    \n",
    "    # Permute x,y\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    permi= np.argsort(perm)\n",
    "    x0,y0= get_xy_by_inds(x,y,permi)\n",
    "                    \n",
    "    train_x = x0[:train_size,:]\n",
    "    train_y = y0[:train_size,:]\n",
    "    \n",
    "    test_x  = x0[train_size:,:]\n",
    "    test_y  = y0[train_size:,:]\n",
    "    \n",
    "    batches = generate_batches(train_size,batch_size)\n",
    "    \n",
    "    return test_x,test_y,train_x,train_y,batches\n",
    "\n",
    "def generate_groundtruthtest(x,y,ground_truth_frac,ground_truth_periods):\n",
    "    # Separate input data into (x,y) pairs for training and (x,y) pairs for ground truth testing\n",
    "    if ground_truth_frac<0 or ground_truth_frac>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*ground_truth_frac/ground_truth_periods))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "        \n",
    "    n_samples_per_period = np.int(np.round(n_samples/ground_truth_periods))\n",
    "    n_test_samples_per_period = np.int(np.round(n_samples_per_period*ground_truth_frac))\n",
    "    n_train_samples_per_period = np.int(np.round(n_samples_per_period - n_test_samples_per_period))\n",
    "                                        \n",
    "    train_inds = np.array([])\n",
    "    test_inds = np.array([])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(ground_truth_periods):\n",
    "        if i!=(ground_truth_periods-1):\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,counter+n_train_samples_per_period))])\n",
    "            counter += n_train_samples_per_period\n",
    "        else:\n",
    "            # Assume n_samples>>ground_truth_periods\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,n_samples))])\n",
    "            counter += (n_samples-counter)\n",
    "    test_inds = [np.int(i) for i in test_inds]\n",
    "    train_inds = [np.int(i) for i in train_inds]\n",
    "    \n",
    "    train_x,train_y   = get_xy_by_inds(x,y,train_inds)\n",
    "    test_x,test_y     = get_xy_by_inds(x,y,test_inds)    \n",
    "    \n",
    "    return test_inds,test_x,test_y,train_inds,train_x,train_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b8ad7077edc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_inds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerate_groundtruthtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2fb1e5bdb070>\u001b[0m in \u001b[0;36mgenerate_groundtruthtest\u001b[0;34m(x, y, ground_truth_frac, ground_truth_periods)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2fb1e5bdb070>\u001b[0m in \u001b[0;36mget_xy_by_inds\u001b[0;34m(x, y, inds)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Get values from 2Dx, 1Dy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# because Python treats 1D arrays differently from ND arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "x = np.array(list(range(0,100000)))\n",
    "y = np.zeros(100000)\n",
    "test_inds,train_inds= generate_groundtruthtest(x,y,0.2,7)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_sign = np.zeros(100000)\n",
    "for i in test_inds:\n",
    "    test_sign[i] = 1\n",
    "    \n",
    "plt.plot(x,test_sign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_deep_learner_arbshape(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,scope=''):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is a LIST of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    num_layers = len(h)\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        with tf.variable_scope('func_learner'):\n",
    "            # 0th hidden layer\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "            # Other hidden layers\n",
    "            for i in range(num_layers-1):\n",
    "                hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h[i],h[i+1],'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "                bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "            # Output layer\n",
    "            weights1= tf.get_variable(name='output_layer_weights',\n",
    "                                  shape=[h[-1],n],\n",
    "                                  initializer=tf.random_normal_initializer(),\n",
    "                                  dtype=DTYPE)\n",
    "            output = tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  batch_size=32, epochs =5, test_fraction=0.1, test_interval=250, initial_learning_rate = 0.02,\n",
    "                  epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 10,\n",
    "                  sgd_switch_epoch = 3,prob_keep = 0.8,\n",
    "                  ground_truth_frac=0.2, ground_truth_periods = 7,\n",
    "                  fname_model_out='/tmp/model.ckpt',\n",
    "                  fname_data_out='/tmp/model_data.h5'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 1}\n",
    "    )\n",
    "    \n",
    "    # Separate data into ground truth test and train data\n",
    "    ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(x_data,y_data,ground_truth_frac,ground_truth_periods)\n",
    "    \n",
    "    with tf.Session(config=config) as sess:        \n",
    "        # Generate test data initially for convenience\n",
    "        test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "        n_samples = len(y_data)\n",
    "        max_steps = batches.shape[1]\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1,scope='model0')\n",
    "        y_pred1, bn_moving_avg_updates1 = func_deep_learner_arbshape(x,input_size,output_size,[64,128,256,128,64,1],is_test,global_step,1,scope='model1')\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            with tf.name_scope('model0'):\n",
    "                mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "            with tf.name_scope('model1'):\n",
    "                mse1 = tf.losses.mean_squared_error(y,y_pred1)\n",
    "\n",
    "        \n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            #learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_ind, epoch_learning_rate_interval, epoch_learning_rate_decay, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step      = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "            train_step1      = tf.train.AdamOptimizer(learning_rate).minimize(mse1, global_step = global_step)\n",
    "            #train_step = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd1 = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse1, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        #ground_truthtest_writer = tf.summary.FileWriter(log_dir + '/ground_truth_test', sess.graph)\n",
    "        train_writer1 = tf.summary.FileWriter(log_dir + '/train1', sess.graph)\n",
    "        test_writer1 = tf.summary.FileWriter(log_dir + '/test1', sess.graph)\n",
    "        #ground_truthtest_writer1 = tf.summary.FileWriter(log_dir + '/ground_truth_test1', sess.graph)\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            # At each epoch, regenerate test data, train data, and batches\n",
    "            test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "            n_batches = batches.shape[1]\n",
    "            n_test = len(test_y)\n",
    "            epoch_init_time = time.time()\n",
    "                \n",
    "            print('Starting Epoch %d, with %d batches' % (j,max_steps))\n",
    "            for i in range(n_batches):\n",
    "                if i % test_interval ==0:\n",
    "                    # Check how our model performs against the epoch test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: ground_truth_test_x, y: ground_truth_test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    # Check how our model performs against the ground truth test data\n",
    "                    #[summary, ground_mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    #ground_truthtest_writer.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    \n",
    "                    # model1\n",
    "                    [summary, mse_val1] = sess.run([merged_summaries,mse1], feed_dict={x: ground_truth_test_x, y: ground_truth_test_y, is_test: True})\n",
    "                    test_writer1.add_summary(summary,i + j*max_steps)\n",
    "                    #[summary, ground_mse_val] = sess.run([merged_summaries,mse1], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    #ground_truthtest_writer1.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    curr_time = time.time()\n",
    "                    print('\\tEpoch %d, Step %04d, Model0 MSE: %4.3e, Model1 MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val,mse_val1, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = get_xy_by_inds(train_x_full,train_y_full,batches[:,i])\n",
    "                    if (np.mod(j,epoch_learning_rate_interval)<sgd_switch_epoch):\n",
    "                        summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                        summary1, _ = sess.run([merged_summaries, train_step1], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    else:\n",
    "                        summary, _ = sess.run([merged_summaries, train_step_sgd], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                        summary1, _ = sess.run([merged_summaries, train_step_sgd1], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates1, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "                    train_writer1.add_summary(summary1,i+ j*max_steps)\n",
    "            # End of epoch, calculate avg stats\n",
    "            epoch_end_time = time.time()\n",
    "            sess.run(increment_epoch_ind)\n",
    "            avg_mrad = np.sqrt(mse_val)*1000\n",
    "            print('Epoch %d time: %4.3fs, average error = %4.3fmrad' % (j,epoch_end_time-epoch_init_time,avg_mrad))\n",
    "        train_writer.close()\n",
    "        test_writer.close()  \n",
    "        train_writer1.close()\n",
    "        test_writer1.close()  \n",
    "        #ground_truthtest_writer.close()\n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))\n",
    "        \n",
    "        # Save model\n",
    "        save_path = saver.save(sess, fname_model_out)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "    print(\"Running inference . . . \")\n",
    "    # Do inference on full data set\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_out)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: True})\n",
    "    \n",
    "    print(\"Serialising data and predictions . . . \")\n",
    "    # Save parameters, data\n",
    "    with h5py.File(fname_data_out,'w') as file:\n",
    "        file.create_dataset('/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "        file.create_dataset('/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "        file.create_dataset('/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "        file.create_dataset('/preds', data=predicted_values.reshape(len(predicted_values)), dtype='float64')\n",
    "        file.create_dataset('/ground_truth_test_inds', data=ground_truth_test_inds)\n",
    "        file.create_dataset('/ground_truth_train_inds', data=ground_truth_train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv_inference(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  fname_model_in='/tmp/model.ckpt'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_in)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: False})\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(acc,phase,fname_model_out='',fname_data_out=''):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    batch_size  = 768\n",
    "    \n",
    "    mccv_steps   = 5000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 1000,sgd_switch_epoch = 750,\n",
    "                            prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(acc,phase,fname):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    \n",
    "    preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n",
    "                            x_data=acc,y_data=phase,fname_model_in=fname)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_in = './ChrisFreierPhDCampaigns/campaign4_proc_1000ms.h5'\n",
    "fname_out = './ChrisFreierPhDCampaigns/campaign4_proc_1000ms_preds_up.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for inference\n",
    "fname_infer_in  = './campaign4_proc_1000ms.h5'\n",
    "fname_infer_out = './campaign4_proc_1000ms_preds_up.h5'\n",
    "with h5py.File(fname_infer_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    #dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    infer_ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    infer_ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    infer_ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    #ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    #ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    #ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference\n",
    "reset_tf()\n",
    "preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4a_HB.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "#shutil.copy(fname_in,fname_out)\n",
    "ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(ai_kup_acc,ai_kup_phase,0.2,5)\n",
    "with h5py.File(fname_infer_out,'w') as file:\n",
    "    file.create_dataset('/ai_kup/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/preds', data=preds_up.reshape(len(preds_up)), dtype='float64')\n",
    "    file.create_dataset('/ai_kup/ground_truth_test_inds', data=np.array(ground_truth_test_inds).reshape(len(ground_truth_test_inds)), dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1712.07628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0, with 156 batches\n",
      "\tEpoch 0, Step 0000, Model0 MSE: 4.000e+00, Model1 MSE: 3.921e-02,  LearnRate: 1.000e-01, Time taken : 12.533s\n",
      "\tEpoch 0, Step 0050, Model0 MSE: 2.923e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 21.146s\n",
      "\tEpoch 0, Step 0100, Model0 MSE: 2.923e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 16.912s\n",
      "\tEpoch 0, Step 0150, Model0 MSE: 1.448e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 16.797s\n",
      "Epoch 0 time: 61.976s, average error = 120.336mrad\n",
      "Starting Epoch 1, with 156 batches\n",
      "\tEpoch 1, Step 0000, Model0 MSE: 1.379e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 9.775s\n",
      "\tEpoch 1, Step 0050, Model0 MSE: 1.178e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 15.929s\n",
      "\tEpoch 1, Step 0100, Model0 MSE: 1.052e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 11.726s\n",
      "\tEpoch 1, Step 0150, Model0 MSE: 1.027e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 11.792s\n",
      "Epoch 1 time: 46.873s, average error = 101.322mrad\n",
      "Starting Epoch 2, with 156 batches\n",
      "\tEpoch 2, Step 0000, Model0 MSE: 1.023e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 6.508s\n",
      "\tEpoch 2, Step 0050, Model0 MSE: 1.023e-02, Model1 MSE: 2.917e-02,  LearnRate: 1.000e-01, Time taken : 18.041s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-82dd77f66f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai_kup_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mai_kup_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_model_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_data_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-3664175b9375>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(acc, phase, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     16\u001b[0m                             \u001b[0mx_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmccv_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                             \u001b[0mepoch_learning_rate_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_learning_rate_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_switch_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                             prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-e80c8dade68d>\u001b[0m in \u001b[0;36mtrain_RBM_complete_mccv\u001b[0;34m(input_size, output_size, hidden_sizes, x_data, y_data, batch_size, epochs, test_fraction, test_interval, initial_learning_rate, epoch_learning_rate_decay, epoch_learning_rate_interval, sgd_switch_epoch, prob_keep, ground_truth_frac, ground_truth_periods, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_interval\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;31m# Check how our model performs against the epoch test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mground_truth_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mground_truth_test_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                     \u001b[0mtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/intel_3.6_TF_1.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4_1000ms.ckpt',fname_data_out='./model_ground_truth_campaign4_1000ms.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel_3.6_TF_1.8]",
   "language": "python",
   "name": "conda-env-intel_3.6_TF_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
