{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math\n",
    "import h5py\n",
    "import shutil\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float64\n",
    "DTYPE_np = np.float64\n",
    "log_dir = '/mnt/dataDrive3/mahasen/tmp/funclearn' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        #with tf.name_scope('biases'):\n",
    "        #    biases = get_biases([output_dim],DTYPE)\n",
    "        #    variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        #logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        logits = tf.matmul(input_tensor, weights,name='logits') # don't need biases if we're using batch norms\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processed campaign4a\n",
    "Matlab used to assign subsequences in `accelerometer` to each entry in `ai_kup` and `ai_kdown`.\n",
    "Subsequence length is 195. ~100000 entries for each of `ai_kup` and `ai_kdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that will generate indices for batches\n",
    "# accounting for the possibility that the size of the dataset\n",
    "# will not be an even multiple of the batch size\n",
    "def generate_batches(N,batch_size=32):\n",
    "    # N is the number of elements in the dataset\n",
    "    if N<batch_size:\n",
    "        raise ValueError('batch_size must be smaller than N')\n",
    "    perm = np.random.permutation(N);\n",
    "    if np.mod(N,batch_size)!=0:\n",
    "        # We need to append to perm so that is is an even multiple of batch_size\n",
    "        perm2= np.random.permutation(N);\n",
    "        perm = np.concatenate((perm,perm2[0:batch_size-np.mod(N,batch_size)]))\n",
    "    \n",
    "    n_batches = np.int(len(perm)/batch_size)\n",
    "    batches = perm.reshape(batch_size,n_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def get_xy_by_inds(x,y,inds):\n",
    "    # Get values from 2Dx, 1Dy arrays\n",
    "    x0 = x[inds,:]\n",
    "    y0 = np.array(list(map(y.__getitem__,inds))) # because Python treats 1D arrays differently from ND arrays\n",
    "    y0 = y0.reshape([len(y0),1])\n",
    "    return x0,y0\n",
    "    \n",
    "def generate_testtrain(x,y,test_fraction,batch_size=32):\n",
    "    # Assumes x.shape = [n_samples,input_size], y.shape = [input_size,]\n",
    "    if test_fraction<0 or test_fraction>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    \n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*test_fraction))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "    train_size = np.int(n_samples-test_size)\n",
    "    \n",
    "    # Permute x,y\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    permi= np.argsort(perm)\n",
    "    x0,y0= get_xy_by_inds(x,y,permi)\n",
    "                    \n",
    "    train_x = x0[:train_size,:]\n",
    "    train_y = y0[:train_size,:]\n",
    "    \n",
    "    test_x  = x0[train_size:,:]\n",
    "    test_y  = y0[train_size:,:]\n",
    "    \n",
    "    batches = generate_batches(train_size,batch_size)\n",
    "    \n",
    "    return test_x,test_y,train_x,train_y,batches\n",
    "\n",
    "def generate_groundtruthtest(x,y,ground_truth_frac,ground_truth_periods):\n",
    "    # Separate input data into (x,y) pairs for training and (x,y) pairs for ground truth testing\n",
    "    if ground_truth_frac<0 or ground_truth_frac>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*ground_truth_frac/ground_truth_periods))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "        \n",
    "    n_samples_per_period = np.int(np.round(n_samples/ground_truth_periods))\n",
    "    n_test_samples_per_period = np.int(np.round(n_samples_per_period*ground_truth_frac))\n",
    "    n_train_samples_per_period = np.int(np.round(n_samples_per_period - n_test_samples_per_period))\n",
    "                                        \n",
    "    train_inds = np.array([])\n",
    "    test_inds = np.array([])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(ground_truth_periods):\n",
    "        if i!=(ground_truth_periods-1):\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,counter+n_train_samples_per_period))])\n",
    "            counter += n_train_samples_per_period\n",
    "        else:\n",
    "            # Assume n_samples>>ground_truth_periods\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,n_samples))])\n",
    "            counter += (n_samples-counter)\n",
    "    test_inds = [np.int(i) for i in test_inds]\n",
    "    train_inds = [np.int(i) for i in train_inds]\n",
    "    \n",
    "    train_x,train_y   = get_xy_by_inds(x,y,train_inds)\n",
    "    test_x,test_y     = get_xy_by_inds(x,y,test_inds)    \n",
    "    \n",
    "    return test_inds,test_x,test_y,train_inds,train_x,train_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_deep_learner_arbshape(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is a LIST of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    num_layers = len(h)\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h[i],h[i+1],'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h[-1],n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  batch_size=32, epochs =5, test_fraction=0.1, test_interval=250, initial_learning_rate = 0.02,\n",
    "                  epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 10,\n",
    "                  sgd_switch_epoch = 3,prob_keep = 0.8,\n",
    "                  ground_truth_frac=0.2, ground_truth_periods = 7,\n",
    "                  fname_model_out='/tmp/model.ckpt',\n",
    "                  fname_data_out='/tmp/model_data.h5'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 1}\n",
    "    )\n",
    "    \n",
    "    # Separate data into ground truth test and train data\n",
    "    ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(x_data,y_data,ground_truth_frac,ground_truth_periods)\n",
    "    \n",
    "    with tf.Session(config=config) as sess:        \n",
    "        # Generate test data initially for convenience\n",
    "        test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "        n_samples = len(y_data)\n",
    "        max_steps = batches.shape[1]\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            #learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_ind, epoch_learning_rate_interval, epoch_learning_rate_decay, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step      = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "            #train_step = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        ground_truthtest_writer = tf.summary.FileWriter(log_dir + '/ground_truth_test', sess.graph)\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            # At each epoch, regenerate test data, train data, and batches\n",
    "            test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "            n_batches = batches.shape[1]\n",
    "            n_test = len(test_y)\n",
    "            epoch_init_time = time.time()\n",
    "                \n",
    "            print('Starting Epoch %d, with %d batches' % (j,max_steps))\n",
    "            for i in range(n_batches):\n",
    "                if i % test_interval ==0:\n",
    "                    # Check how our model performs against the epoch test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: ground_truth_test_x, y: ground_truth_test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    # Check how our model performs against the ground truth test data\n",
    "                    [summary, ground_mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    ground_truthtest_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('\\tEpoch %d, Step %04d, MSE: %4.3e, Ground truth MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val,ground_mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = get_xy_by_inds(train_x_full,train_y_full,batches[:,i])\n",
    "                    if (np.mod(j,epoch_learning_rate_interval)<sgd_switch_epoch):\n",
    "                        summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    else:\n",
    "                        summary, _ = sess.run([merged_summaries, train_step_sgd], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            # End of epoch, calculate avg stats\n",
    "            epoch_end_time = time.time()\n",
    "            sess.run(increment_epoch_ind)\n",
    "            avg_mrad = np.sqrt(mse_val)*1000\n",
    "            print('Epoch %d time: %4.3fs, average error = %4.3fmrad' % (j,epoch_end_time-epoch_init_time,avg_mrad))\n",
    "        train_writer.close()\n",
    "        test_writer.close()  \n",
    "        ground_truthtest_writer.close()\n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))\n",
    "        \n",
    "        # Save model\n",
    "        print('Saving to '+fname_model_out+' ...')\n",
    "        save_path = saver.save(sess, fname_model_out)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "#     print(\"Running inference . . . \")\n",
    "#     # Do inference on full data set\n",
    "#     config = tf.ConfigProto(\n",
    "#         device_count = {'GPU': 0}\n",
    "#     )\n",
    "    \n",
    "#     tf.reset_default_graph()\n",
    "#     with tf.Session(config=config) as sess:\n",
    "#         with tf.name_scope('input'):\n",
    "#             # Placeholder variables which will be fed data at train time\n",
    "#             x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "#             y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "#         with tf.name_scope('control_inputs'):\n",
    "#             global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "#             epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "#             increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "#             is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "#         # Predictions of the NN\n",
    "#         y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "#         # Restore all the variables.\n",
    "#         saver = tf.train.Saver()\n",
    "#         saver.restore(sess,  fname_model_out)\n",
    "        \n",
    "#         # Run inference\n",
    "#         predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: True})\n",
    "    \n",
    "#     print(\"Serialising data and predictions . . . \")\n",
    "#     # Save parameters, data\n",
    "#     with h5py.File(fname_data_out,'w') as file:\n",
    "#         file.create_dataset('/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "#         file.create_dataset('/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "#         file.create_dataset('/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "#         file.create_dataset('/preds', data=predicted_values.reshape(len(predicted_values)), dtype='float64')\n",
    "#         file.create_dataset('/ground_truth_test_inds', data=ground_truth_test_inds)\n",
    "#         file.create_dataset('/ground_truth_train_inds', data=ground_truth_train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv_inference(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  fname_model_in='/tmp/model.ckpt'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    reset_tf()\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_in)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: False})\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(acc,phase,fname_model_out='',fname_data_out=''):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    batch_size  = 768\n",
    "    \n",
    "    mccv_steps   = 5000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 1000,sgd_switch_epoch = 750,\n",
    "                            prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_opt(acc,phase,batch_size,mccv_steps,epoch_learning_rate_interval,alpha,prob_keep,fname_model_out='./opt_model.ckpt',fname_data_out='./opt_model_data.h5'):\n",
    "    \n",
    "    #acc\n",
    "    #phase\n",
    "    fname_model_out='./opt_model.ckpt'\n",
    "    fname_data_out='./opt_model_data.h5'\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = (np.array([128,64,32,16,8])*alpha).astype(np.int) # scale width of model\n",
    "    #batch_size  = 768\n",
    "    \n",
    "    #mccv_steps   = 5000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = epoch_learning_rate_interval,sgd_switch_epoch = np.int(np.round(0.5*epoch_learning_rate_interval)),\n",
    "                            prob_keep = prob_keep,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n",
    "    \n",
    "    # Get predicted values\n",
    "    preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n",
    "                            x_data=acc,y_data=phase,fname_model_in=fname_model_out)\n",
    "    \n",
    "    # Compute average error in mrad:\n",
    "    mse = np.sqrt(np.mean(np.square(np.subtract(preds.flatten(),phase.flatten()))))*1000\n",
    "    \n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_func_maker(acc,phase):\n",
    "    \n",
    "    \n",
    "    def min_func(batch_size,epoch_learning_rate_interval,alpha,prob_keep):\n",
    "        # Let's fix this so that we can bound total time\n",
    "        mccv_steps = 25\n",
    "        \n",
    "        # For some reason dlib plays fast and loose with typing?\n",
    "        batch_size = np.int(batch_size)\n",
    "        mccv_steps = np.int(mccv_steps)\n",
    "        epoch_learning_rate_interval = np.int(epoch_learning_rate_interval)\n",
    "        alpha = np.int(alpha)\n",
    "        \n",
    "        print('batch_size=%d, mccv_steps=%d, epoch_learning_rate=%d, alpha=%d, prob_keep=%4.3f\\n'%(batch_size,mccv_steps,epoch_learning_rate_interval,alpha,prob_keep))\n",
    "        \n",
    "        reset_tf()\n",
    "        try:\n",
    "            min_val = train_model_opt(acc,phase,batch_size,mccv_steps,epoch_learning_rate_interval,alpha,prob_keep)\n",
    "            print('MSE = %4.3fmrad\\n'%min_val)\n",
    "        except:\n",
    "            min_val = np.infty\n",
    "        return min_val\n",
    "        \n",
    "    return min_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(acc,phase,fname):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,64,32,16,8]\n",
    "    \n",
    "    preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n",
    "                            x_data=acc,y_data=phase,fname_model_in=fname)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_in = './ChrisFreierPhDCampaigns/campaign4_proc_1000ms.h5'\n",
    "fname_out = './ChrisFreierPhDCampaigns/campaign4_proc_1000ms_preds_up.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "#     ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "#     ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "#     ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = train_model_opt(ai_kup_acc,ai_kup_phase,256,1000,1000,2,0.7,fname_model_out='./opt_model_0.ckpt',fname_data_out='./opt_model_data_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_min, mccv_steps_min, epoch_learning_rate_interval_min, alpha_min, prob_keep_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise!\n",
    "min_func = min_func_maker(ai_kup_acc,ai_kup_phase)\n",
    "\n",
    "batch_size_min, mccv_steps_min, epoch_learning_rate_interval_min, alpha_min, prob_keep_min = dlib.find_min_global(min_func,\n",
    "                                                                                                                 [8,1,1,0.4],\n",
    "                                                                                                                 [512,25,3,1],\n",
    "                                                                                                                 [True,True,True,False],\n",
    "                                                                                                                 50)\n",
    "#dlib.find_min_global(min_func,[8,512],[1,1000],[1,500],[1,3],[0.4,1],[True,True,True,True,False],50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([128,64,32,16,8])*2.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_func = min_func_maker(ai_kup_acc,ai_kup_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_func(512,1,10,1,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for inference\n",
    "fname_infer_in  = './campaign4_proc_1000ms.h5'\n",
    "fname_infer_out = './campaign4_proc_1000ms_preds_up.h5'\n",
    "with h5py.File(fname_infer_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    #dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    infer_ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    infer_ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    infer_ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    #ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    #ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    #ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=263, mccv_steps=25, epoch_learning_rate=23, alpha=2, prob_keep=0.712\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference\n",
    "reset_tf()\n",
    "preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4a_HB.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "#shutil.copy(fname_in,fname_out)\n",
    "ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(ai_kup_acc,ai_kup_phase,0.2,5)\n",
    "with h5py.File(fname_infer_out,'w') as file:\n",
    "    file.create_dataset('/ai_kup/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/preds', data=preds_up.reshape(len(preds_up)), dtype='float64')\n",
    "    file.create_dataset('/ai_kup/ground_truth_test_inds', data=np.array(ground_truth_test_inds).reshape(len(ground_truth_test_inds)), dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1712.07628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4_100ms.ckpt',fname_data_out='./model_ground_truth_campaign4_100ms.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin,cos,pi,exp,sqrt\n",
    "def holder_table(x0,x1):\n",
    "    return -abs(sin(x0)*cos(x1)*exp(abs(1-sqrt(x0*x0+x1*x1)/pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal inputs to holder_table().  The print statements that follow\n",
    "# show that find_min_global() finds the optimal settings to high precision.\n",
    "x,y = dlib.find_min_global(holder_table, \n",
    "                           [-10,-10],  # Lower bound constraints on x0 and x1 respectively\n",
    "                           [10,10],    # Upper bound constraints on x0 and x1 respectively\n",
    "                           80)         # The number of times find_min_global() will call holder_table()\n",
    "\n",
    "print(\"optimal inputs: {}\".format(x));\n",
    "print(\"optimal output: {}\".format(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel_3.6_TF_1.8]",
   "language": "python",
   "name": "conda-env-intel_3.6_TF_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
