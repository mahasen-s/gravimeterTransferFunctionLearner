{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math\n",
    "import h5py\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float64\n",
    "DTYPE_np = np.float64\n",
    "log_dir = '/home/mahasen/tf_logs/transferFuncLearner' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        #with tf.name_scope('biases'):\n",
    "        #    biases = get_biases([output_dim],DTYPE)\n",
    "        #    variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        #logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        logits = tf.matmul(input_tensor, weights,name='logits') # don't need biases if we're using batch norms\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processed campaign4a\n",
    "Matlab used to assign subsequences in `accelerometer` to each entry in `ai_kup` and `ai_kdown`.\n",
    "Subsequence length is 195. ~100000 entries for each of `ai_kup` and `ai_kdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that will generate indices for batches\n",
    "# accounting for the possibility that the size of the dataset\n",
    "# will not be an even multiple of the batch size\n",
    "def generate_batches(N,batch_size=32):\n",
    "    # N is the number of elements in the dataset\n",
    "    if N<batch_size:\n",
    "        raise ValueError('batch_size must be smaller than N')\n",
    "    perm = np.random.permutation(N);\n",
    "    if np.mod(N,batch_size)!=0:\n",
    "        # We need to append to perm so that is is an even multiple of batch_size\n",
    "        perm2= np.random.permutation(N);\n",
    "        perm = np.concatenate((perm,perm2[0:batch_size-np.mod(N,batch_size)]))\n",
    "    \n",
    "    n_batches = np.int(len(perm)/batch_size)\n",
    "    batches = perm.reshape(batch_size,n_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def get_xy_by_inds(x,y,inds):\n",
    "    # Get values from 2Dx, 1Dy arrays\n",
    "    x0 = x[inds,:]\n",
    "    y0 = np.array(list(map(y.__getitem__,inds))) # because Python treats 1D arrays differently from ND arrays\n",
    "    y0 = y0.reshape([len(y0),1])\n",
    "    return x0,y0\n",
    "    \n",
    "def generate_testtrain(x,y,test_fraction,batch_size=32):\n",
    "    # Assumes x.shape = [n_samples,input_size], y.shape = [input_size,]\n",
    "    if test_fraction<0 or test_fraction>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    \n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*test_fraction))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "    train_size = np.int(n_samples-test_size)\n",
    "    \n",
    "    # Permute x,y\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    permi= np.argsort(perm)\n",
    "    x0,y0= get_xy_by_inds(x,y,permi)\n",
    "                    \n",
    "    train_x = x0[:train_size,:]\n",
    "    train_y = y0[:train_size,:]\n",
    "    \n",
    "    test_x  = x0[train_size:,:]\n",
    "    test_y  = y0[train_size:,:]\n",
    "    \n",
    "    batches = generate_batches(train_size,batch_size)\n",
    "    \n",
    "    return test_x,test_y,train_x,train_y,batches\n",
    "\n",
    "def generate_groundtruthtest(x,y,ground_truth_frac,ground_truth_periods):\n",
    "    # Separate input data into (x,y) pairs for training and (x,y) pairs for ground truth testing\n",
    "    if ground_truth_frac<0 or ground_truth_frac>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*ground_truth_frac/ground_truth_periods))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "        \n",
    "    n_samples_per_period = np.int(np.round(n_samples/ground_truth_periods))\n",
    "    n_test_samples_per_period = np.int(np.round(n_samples_per_period*ground_truth_frac))\n",
    "    n_train_samples_per_period = np.int(np.round(n_samples_per_period - n_test_samples_per_period))\n",
    "                                        \n",
    "    train_inds = np.array([])\n",
    "    test_inds = np.array([])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(ground_truth_periods):\n",
    "        if i!=(ground_truth_periods-1):\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,counter+n_train_samples_per_period))])\n",
    "            counter += n_train_samples_per_period\n",
    "        else:\n",
    "            # Assume n_samples>>ground_truth_periods\n",
    "            test_inds = np.concatenate([test_inds,list(range(counter,counter+n_test_samples_per_period))])\n",
    "            counter += n_test_samples_per_period\n",
    "            train_inds = np.concatenate([train_inds,list(range(counter,n_samples))])\n",
    "            counter += (n_samples-counter)\n",
    "    test_inds = [np.int(i) for i in test_inds]\n",
    "    train_inds = [np.int(i) for i in train_inds]\n",
    "    \n",
    "    train_x,train_y   = get_xy_by_inds(x,y,train_inds)\n",
    "    test_x,test_y     = get_xy_by_inds(x,y,test_inds)    \n",
    "    \n",
    "    return test_inds,test_x,test_y,train_inds,train_x,train_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5362c81743c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_inds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgenerate_groundtruthtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3849bfcfcdf>\u001b[0m in \u001b[0;36mgenerate_groundtruthtest\u001b[0;34m(x, y, ground_truth_frac, ground_truth_periods)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mtrain_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b3849bfcfcdf>\u001b[0m in \u001b[0;36mget_xy_by_inds\u001b[0;34m(x, y, inds)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_xy_by_inds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Get values from 2Dx, 1Dy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# because Python treats 1D arrays differently from ND arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "x = np.array(list(range(0,100000)))\n",
    "y = np.zeros(100000)\n",
    "test_inds,train_inds= generate_groundtruthtest(x,y,0.2,7)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_sign = np.zeros(100000)\n",
    "for i in test_inds:\n",
    "    test_sign[i] = 1\n",
    "    \n",
    "plt.plot(x,test_sign)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_deep_learner_arbshape(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is a LIST of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    num_layers = len(h)\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h[i],h[i+1],'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h[-1],n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  batch_size=32, epochs =5, test_fraction=0.1, test_interval=250, initial_learning_rate = 0.02,\n",
    "                  epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 10,\n",
    "                  sgd_switch_epoch = 3,prob_keep = 0.8,\n",
    "                  ground_truth_frac=0.2, ground_truth_periods = 7,\n",
    "                  fname_model_out='/tmp/model.ckpt',\n",
    "                  fname_data_out='/tmp/model_data.h5'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 2}\n",
    "    )\n",
    "    \n",
    "    # Separate data into ground truth test and train data\n",
    "    ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(x_data,y_data,ground_truth_frac,ground_truth_periods)\n",
    "    \n",
    "    with tf.Session(config=config) as sess:        \n",
    "        # Generate test data initially for convenience\n",
    "        test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "        n_samples = len(y_data)\n",
    "        max_steps = batches.shape[1]\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            #learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_ind, epoch_learning_rate_interval, epoch_learning_rate_decay, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step      = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "            #train_step = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd = tf.train.MomentumOptimizer(learning_rate,0.9,use_nesterov=True).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        ground_truthtest_writer = tf.summary.FileWriter(log_dir + '/ground_truth_test', sess.graph)\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            # At each epoch, regenerate test data, train data, and batches\n",
    "            test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(ground_truth_train_x,ground_truth_train_y,test_fraction,batch_size)\n",
    "            n_batches = batches.shape[1]\n",
    "            n_test = len(test_y)\n",
    "            epoch_init_time = time.time()\n",
    "                \n",
    "            print('Starting Epoch %d, with %d batches' % (j,max_steps))\n",
    "            for i in range(n_batches):\n",
    "                if i % test_interval ==0:\n",
    "                    # Check how our model performs against the epoch test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: ground_truth_test_x, y: ground_truth_test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    \n",
    "                    # Check how our model performs against the ground truth test data\n",
    "                    [summary, ground_mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    ground_truthtest_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('\\tEpoch %d, Step %04d, MSE: %4.3e, Ground truth MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val,ground_mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = get_xy_by_inds(train_x_full,train_y_full,batches[:,i])\n",
    "                    if (np.mod(j,epoch_learning_rate_interval)<sgd_switch_epoch):\n",
    "                        summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    else:\n",
    "                        summary, _ = sess.run([merged_summaries, train_step_sgd], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            # End of epoch, calculate avg stats\n",
    "            epoch_end_time = time.time()\n",
    "            sess.run(increment_epoch_ind)\n",
    "            avg_mrad = np.sqrt(mse_val)*1000\n",
    "            print('Epoch %d time: %4.3fs, average error = %4.3fmrad' % (j,epoch_end_time-epoch_init_time,avg_mrad))\n",
    "        train_writer.close()\n",
    "        test_writer.close()  \n",
    "        ground_truthtest_writer.close()\n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))\n",
    "        \n",
    "        # Save model\n",
    "        save_path = saver.save(sess, fname_model_out)\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "    print(\"Running inference . . . \")\n",
    "    # Do inference on full data set\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session(config=config) as sess:\n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_out)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: True})\n",
    "    \n",
    "    print(\"Serialising data and predictions . . . \")\n",
    "    # Save parameters, data\n",
    "    with h5py.File(fname_data_out,'w') as file:\n",
    "        file.create_dataset('/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "        file.create_dataset('/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "        file.create_dataset('/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "        file.create_dataset('/preds', data=predicted_values.reshape(len(predicted_values)), dtype='float64')\n",
    "        file.create_dataset('/ground_truth_test_inds', data=ground_truth_test_inds)\n",
    "        file.create_dataset('/ground_truth_train_inds', data=ground_truth_train_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv_inference(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  fname_model_in='/tmp/model.ckpt'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "        \n",
    "        # Restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess,  fname_model_in)\n",
    "        \n",
    "        # Run inference\n",
    "        predicted_values = sess.run(y_pred, feed_dict={x: x_data, is_test: False})\n",
    "    return predicted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(acc,phase,fname_model_out='',fname_data_out=''):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    batch_size  = 768\n",
    "    \n",
    "    mccv_steps   = 5000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 1000,sgd_switch_epoch = 750,\n",
    "                            prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(acc,phase,fname):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    \n",
    "    preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n",
    "                            x_data=acc,y_data=phase,fname_model_in=fname)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_test(acc,phase,fname_model_out='',fname_data_out=''):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,96,64,32,16,8]\n",
    "    batch_size  = 768\n",
    "    \n",
    "    mccv_steps   = 2\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    # sgd_switch_epoch now means that if mod(epoch_ind,epoch_learning_rate_interval)>sgd_switch_epoch, then SGD is used\n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 1000,sgd_switch_epoch = 750,\n",
    "                            prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 5,fname_model_out=fname_model_out,fname_data_out=fname_data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_in = './campaign4a_proc.h5'\n",
    "fname_out = './campaign4a_proc_preds_up.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for inference\n",
    "fname_infer_in  = './campaign4_proc_1000ms.h5'\n",
    "fname_infer_out = './campaign4_proc_1000ms_preds_up.h5'\n",
    "with h5py.File(fname_infer_in,'r') as f0:\n",
    "    dset_up = f0['/ai_kup']\n",
    "    #dset_down = f0['/ai_kdown']\n",
    "    \n",
    "    infer_ai_kup_phase = np.array(list(dset_up['phase']))\n",
    "    infer_ai_kup_acc = np.array(list(dset_up['acc']))\n",
    "    infer_ai_kup_timestamp = np.array(list(dset_up['timestamp']))\n",
    "\n",
    "    #ai_kdown_phase = np.array(list(dset_down['phase']))\n",
    "    #ai_kdown_acc = np.array(list(dset_down['acc']))\n",
    "    #ai_kdown_timestamp = np.array(list(dset_down['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_ground_truth_campaign4a_HB.ckpt\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ground_truth_campaign4a_HB.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_INT32, DT_BOOL, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-8eb6a1b1d879>\", line 3, in <module>\n    preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4a_HB.ckpt')\n  File \"<ipython-input-19-67a8f1ba7a3c>\", line 11, in run_model\n    x_data=acc,y_data=phase,fname_model_in=fname)\n  File \"<ipython-input-17-5d61a20ecce4>\", line 26, in train_RBM_complete_mccv_inference\n    saver = tf.train.Saver()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ground_truth_campaign4a_HB.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_INT32, DT_BOOL, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ground_truth_campaign4a_HB.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_INT32, DT_BOOL, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-8eb6a1b1d879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Do inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreset_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpreds_up\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_ai_kup_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfer_ai_kup_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4a_HB.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-67a8f1ba7a3c>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(acc, phase, fname)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     preds = train_RBM_complete_mccv_inference(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,\n\u001b[0;32m---> 11\u001b[0;31m                             x_data=acc,y_data=phase,fname_model_in=fname)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-5d61a20ecce4>\u001b[0m in \u001b[0;36mtrain_RBM_complete_mccv_inference\u001b[0;34m(input_size, output_size, hidden_sizes, x_data, y_data, fname_model_in)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Restore all the variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfname_model_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0mshould_reraise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mshould_reraise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mexception_traceback\u001b[0m  \u001b[0;31m# avoid reference cycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1750\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1752\u001b[0;31m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1753\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m       \u001b[0mexception_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ground_truth_campaign4a_HB.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_INT32, DT_BOOL, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-33-8eb6a1b1d879>\", line 3, in <module>\n    preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4a_HB.ckpt')\n  File \"<ipython-input-19-67a8f1ba7a3c>\", line 11, in run_model\n    x_data=acc,y_data=phase,fname_model_in=fname)\n  File \"<ipython-input-17-5d61a20ecce4>\", line 26, in train_RBM_complete_mccv_inference\n    saver = tf.train.Saver()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1284, in __init__\n    self.build()\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 781, in _build_internal\n    restore_sequentially, reshape)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 400, in _AddRestoreOps\n    restore_sequentially)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./model_ground_truth_campaign4a_HB.ckpt\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_INT32, DT_INT32, DT_BOOL, DT_DOUBLE, DT_DOUBLE, ..., DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "# Do inference\n",
    "reset_tf()\n",
    "preds_up= run_model(infer_ai_kup_acc,infer_ai_kup_phase,'./model_ground_truth_campaign4a_HB.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "#shutil.copy(fname_in,fname_out)\n",
    "ground_truth_test_inds,ground_truth_test_x,ground_truth_test_y,ground_truth_train_inds,ground_truth_train_x,ground_truth_train_y = generate_groundtruthtest(ai_kup_acc,ai_kup_phase,0.2,5)\n",
    "with h5py.File(fname_infer_out,'w') as file:\n",
    "    file.create_dataset('/ai_kup/phase',data=infer_ai_kup_phase,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/acc',data=infer_ai_kup_acc,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/timestamp',data=infer_ai_kup_timestamp,dtype='float64')\n",
    "    file.create_dataset('/ai_kup/preds', data=preds_up.reshape(len(preds_up)), dtype='float64')\n",
    "    file.create_dataset('/ai_kup/ground_truth_test_inds', data=np.array(ground_truth_test_inds).reshape(len(ground_truth_test_inds)), dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1712.07628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0, with 85 batches\n",
      "\tEpoch 0, Step 0000, MSE: 2.704e+01, Ground truth MSE: 2.702e+01,  LearnRate: 1.000e-01, Time taken : 1.671s\n"
     ]
    }
   ],
   "source": [
    "train_model_test(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4a_100ms_HB.ckpt',fname_data_out='./model_ground_truth_campaign4a_100ms_HB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0, with 156 batches\n",
      "\tEpoch 0, Step 0000, MSE: 3.204e+01, Ground truth MSE: 3.213e+01,  LearnRate: 1.000e-01, Time taken : 3.301s\n",
      "\tEpoch 0, Step 0050, MSE: 3.255e-02, Ground truth MSE: 3.330e-02,  LearnRate: 1.000e-01, Time taken : 7.288s\n",
      "\tEpoch 0, Step 0100, MSE: 3.012e-02, Ground truth MSE: 3.091e-02,  LearnRate: 1.000e-01, Time taken : 5.682s\n",
      "\tEpoch 0, Step 0150, MSE: 2.988e-02, Ground truth MSE: 3.070e-02,  LearnRate: 1.000e-01, Time taken : 4.984s\n",
      "Epoch 0 time: 23.693s, average error = 172.850mrad\n",
      "Starting Epoch 1, with 156 batches\n",
      "\tEpoch 1, Step 0000, MSE: 2.975e-02, Ground truth MSE: 3.084e-02,  LearnRate: 1.000e-01, Time taken : 5.449s\n",
      "\tEpoch 1, Step 0050, MSE: 2.502e-02, Ground truth MSE: 2.588e-02,  LearnRate: 1.000e-01, Time taken : 5.604s\n",
      "\tEpoch 1, Step 0100, MSE: 1.813e-02, Ground truth MSE: 1.830e-02,  LearnRate: 1.000e-01, Time taken : 5.391s\n",
      "\tEpoch 1, Step 0150, MSE: 1.364e-02, Ground truth MSE: 1.382e-02,  LearnRate: 1.000e-01, Time taken : 5.371s\n",
      "Epoch 1 time: 21.880s, average error = 116.789mrad\n",
      "Starting Epoch 2, with 156 batches\n",
      "\tEpoch 2, Step 0000, MSE: 1.159e-02, Ground truth MSE: 1.183e-02,  LearnRate: 1.000e-01, Time taken : 5.795s\n",
      "\tEpoch 2, Step 0050, MSE: 1.086e-02, Ground truth MSE: 1.102e-02,  LearnRate: 1.000e-01, Time taken : 4.928s\n",
      "\tEpoch 2, Step 0100, MSE: 1.020e-02, Ground truth MSE: 1.042e-02,  LearnRate: 1.000e-01, Time taken : 4.974s\n",
      "\tEpoch 2, Step 0150, MSE: 9.699e-03, Ground truth MSE: 9.889e-03,  LearnRate: 1.000e-01, Time taken : 5.387s\n",
      "Epoch 2 time: 20.252s, average error = 98.486mrad\n",
      "Starting Epoch 3, with 156 batches\n",
      "\tEpoch 3, Step 0000, MSE: 1.049e-02, Ground truth MSE: 1.036e-02,  LearnRate: 1.000e-01, Time taken : 5.302s\n",
      "\tEpoch 3, Step 0050, MSE: 9.562e-03, Ground truth MSE: 9.786e-03,  LearnRate: 1.000e-01, Time taken : 4.982s\n",
      "\tEpoch 3, Step 0100, MSE: 9.409e-03, Ground truth MSE: 9.788e-03,  LearnRate: 1.000e-01, Time taken : 5.505s\n",
      "\tEpoch 3, Step 0150, MSE: 9.260e-03, Ground truth MSE: 9.426e-03,  LearnRate: 1.000e-01, Time taken : 5.363s\n",
      "Epoch 3 time: 20.893s, average error = 96.227mrad\n",
      "Starting Epoch 4, with 156 batches\n",
      "\tEpoch 4, Step 0000, MSE: 9.310e-03, Ground truth MSE: 9.734e-03,  LearnRate: 1.000e-01, Time taken : 5.452s\n",
      "\tEpoch 4, Step 0050, MSE: 9.243e-03, Ground truth MSE: 9.669e-03,  LearnRate: 1.000e-01, Time taken : 5.485s\n",
      "\tEpoch 4, Step 0100, MSE: 9.073e-03, Ground truth MSE: 9.587e-03,  LearnRate: 1.000e-01, Time taken : 5.040s\n",
      "\tEpoch 4, Step 0150, MSE: 9.335e-03, Ground truth MSE: 9.883e-03,  LearnRate: 1.000e-01, Time taken : 4.910s\n",
      "Epoch 4 time: 20.350s, average error = 96.617mrad\n",
      "Starting Epoch 5, with 156 batches\n",
      "\tEpoch 5, Step 0000, MSE: 9.156e-03, Ground truth MSE: 9.679e-03,  LearnRate: 1.000e-01, Time taken : 5.201s\n",
      "\tEpoch 5, Step 0050, MSE: 9.266e-03, Ground truth MSE: 9.864e-03,  LearnRate: 1.000e-01, Time taken : 5.175s\n",
      "\tEpoch 5, Step 0100, MSE: 8.976e-03, Ground truth MSE: 9.543e-03,  LearnRate: 1.000e-01, Time taken : 5.016s\n",
      "\tEpoch 5, Step 0150, MSE: 9.318e-03, Ground truth MSE: 9.862e-03,  LearnRate: 1.000e-01, Time taken : 5.236s\n",
      "Epoch 5 time: 20.570s, average error = 96.529mrad\n",
      "Starting Epoch 6, with 156 batches\n",
      "\tEpoch 6, Step 0000, MSE: 9.363e-03, Ground truth MSE: 9.453e-03,  LearnRate: 1.000e-01, Time taken : 5.493s\n",
      "\tEpoch 6, Step 0050, MSE: 9.116e-03, Ground truth MSE: 9.325e-03,  LearnRate: 1.000e-01, Time taken : 5.141s\n",
      "\tEpoch 6, Step 0100, MSE: 9.097e-03, Ground truth MSE: 9.332e-03,  LearnRate: 1.000e-01, Time taken : 5.055s\n",
      "\tEpoch 6, Step 0150, MSE: 9.267e-03, Ground truth MSE: 9.402e-03,  LearnRate: 1.000e-01, Time taken : 5.235s\n",
      "Epoch 6 time: 20.316s, average error = 96.267mrad\n",
      "Starting Epoch 7, with 156 batches\n",
      "\tEpoch 7, Step 0000, MSE: 8.952e-03, Ground truth MSE: 9.354e-03,  LearnRate: 1.000e-01, Time taken : 5.178s\n",
      "\tEpoch 7, Step 0050, MSE: 9.031e-03, Ground truth MSE: 9.538e-03,  LearnRate: 1.000e-01, Time taken : 5.123s\n",
      "\tEpoch 7, Step 0100, MSE: 8.970e-03, Ground truth MSE: 9.413e-03,  LearnRate: 1.000e-01, Time taken : 5.168s\n",
      "\tEpoch 7, Step 0150, MSE: 8.906e-03, Ground truth MSE: 9.377e-03,  LearnRate: 1.000e-01, Time taken : 5.266s\n",
      "Epoch 7 time: 20.430s, average error = 94.369mrad\n",
      "Starting Epoch 8, with 156 batches\n",
      "\tEpoch 8, Step 0000, MSE: 8.918e-03, Ground truth MSE: 8.981e-03,  LearnRate: 1.000e-01, Time taken : 5.270s\n",
      "\tEpoch 8, Step 0050, MSE: 8.987e-03, Ground truth MSE: 9.062e-03,  LearnRate: 1.000e-01, Time taken : 5.071s\n",
      "\tEpoch 8, Step 0100, MSE: 8.924e-03, Ground truth MSE: 8.984e-03,  LearnRate: 1.000e-01, Time taken : 4.830s\n",
      "\tEpoch 8, Step 0150, MSE: 8.871e-03, Ground truth MSE: 8.948e-03,  LearnRate: 1.000e-01, Time taken : 5.195s\n",
      "Epoch 8 time: 19.929s, average error = 94.186mrad\n",
      "Starting Epoch 9, with 156 batches\n",
      "\tEpoch 9, Step 0000, MSE: 8.980e-03, Ground truth MSE: 9.194e-03,  LearnRate: 1.000e-01, Time taken : 5.208s\n",
      "\tEpoch 9, Step 0050, MSE: 8.918e-03, Ground truth MSE: 9.128e-03,  LearnRate: 1.000e-01, Time taken : 4.989s\n",
      "\tEpoch 9, Step 0100, MSE: 8.983e-03, Ground truth MSE: 9.252e-03,  LearnRate: 1.000e-01, Time taken : 5.198s\n",
      "\tEpoch 9, Step 0150, MSE: 8.988e-03, Ground truth MSE: 9.207e-03,  LearnRate: 1.000e-01, Time taken : 4.846s\n",
      "Epoch 9 time: 19.948s, average error = 94.807mrad\n",
      "Starting Epoch 10, with 156 batches\n",
      "\tEpoch 10, Step 0000, MSE: 8.985e-03, Ground truth MSE: 9.220e-03,  LearnRate: 1.000e-01, Time taken : 5.262s\n",
      "\tEpoch 10, Step 0050, MSE: 9.006e-03, Ground truth MSE: 9.315e-03,  LearnRate: 1.000e-01, Time taken : 5.421s\n",
      "\tEpoch 10, Step 0100, MSE: 8.826e-03, Ground truth MSE: 9.117e-03,  LearnRate: 1.000e-01, Time taken : 5.371s\n",
      "\tEpoch 10, Step 0150, MSE: 9.007e-03, Ground truth MSE: 9.309e-03,  LearnRate: 1.000e-01, Time taken : 5.441s\n",
      "Epoch 10 time: 21.157s, average error = 94.903mrad\n",
      "Starting Epoch 11, with 156 batches\n",
      "\tEpoch 11, Step 0000, MSE: 8.925e-03, Ground truth MSE: 9.345e-03,  LearnRate: 1.000e-01, Time taken : 5.258s\n",
      "\tEpoch 11, Step 0050, MSE: 9.639e-03, Ground truth MSE: 1.005e-02,  LearnRate: 1.000e-01, Time taken : 5.681s\n",
      "\tEpoch 11, Step 0100, MSE: 9.061e-03, Ground truth MSE: 9.595e-03,  LearnRate: 1.000e-01, Time taken : 5.308s\n",
      "\tEpoch 11, Step 0150, MSE: 8.973e-03, Ground truth MSE: 9.556e-03,  LearnRate: 1.000e-01, Time taken : 5.070s\n",
      "Epoch 11 time: 20.986s, average error = 94.727mrad\n",
      "Starting Epoch 12, with 156 batches\n",
      "\tEpoch 12, Step 0000, MSE: 8.936e-03, Ground truth MSE: 9.426e-03,  LearnRate: 1.000e-01, Time taken : 5.342s\n",
      "\tEpoch 12, Step 0050, MSE: 8.880e-03, Ground truth MSE: 9.407e-03,  LearnRate: 1.000e-01, Time taken : 5.265s\n",
      "\tEpoch 12, Step 0100, MSE: 8.860e-03, Ground truth MSE: 9.383e-03,  LearnRate: 1.000e-01, Time taken : 5.031s\n",
      "\tEpoch 12, Step 0150, MSE: 8.856e-03, Ground truth MSE: 9.421e-03,  LearnRate: 1.000e-01, Time taken : 5.019s\n",
      "Epoch 12 time: 20.291s, average error = 94.107mrad\n",
      "Starting Epoch 13, with 156 batches\n",
      "\tEpoch 13, Step 0000, MSE: 8.878e-03, Ground truth MSE: 9.302e-03,  LearnRate: 1.000e-01, Time taken : 5.274s\n",
      "\tEpoch 13, Step 0050, MSE: 8.814e-03, Ground truth MSE: 9.297e-03,  LearnRate: 1.000e-01, Time taken : 5.370s\n",
      "\tEpoch 13, Step 0100, MSE: 8.869e-03, Ground truth MSE: 9.332e-03,  LearnRate: 1.000e-01, Time taken : 5.216s\n",
      "\tEpoch 13, Step 0150, MSE: 9.093e-03, Ground truth MSE: 9.538e-03,  LearnRate: 1.000e-01, Time taken : 5.539s\n",
      "Epoch 13 time: 21.053s, average error = 95.355mrad\n",
      "Starting Epoch 14, with 156 batches\n",
      "\tEpoch 14, Step 0000, MSE: 8.826e-03, Ground truth MSE: 8.959e-03,  LearnRate: 1.000e-01, Time taken : 5.232s\n",
      "\tEpoch 14, Step 0050, MSE: 8.752e-03, Ground truth MSE: 8.952e-03,  LearnRate: 1.000e-01, Time taken : 5.369s\n",
      "\tEpoch 14, Step 0100, MSE: 8.905e-03, Ground truth MSE: 9.082e-03,  LearnRate: 1.000e-01, Time taken : 5.099s\n",
      "\tEpoch 14, Step 0150, MSE: 8.794e-03, Ground truth MSE: 9.074e-03,  LearnRate: 1.000e-01, Time taken : 5.347s\n",
      "Epoch 14 time: 20.829s, average error = 93.774mrad\n",
      "Starting Epoch 15, with 156 batches\n",
      "\tEpoch 15, Step 0000, MSE: 8.866e-03, Ground truth MSE: 9.259e-03,  LearnRate: 1.000e-01, Time taken : 5.282s\n",
      "\tEpoch 15, Step 0050, MSE: 9.017e-03, Ground truth MSE: 9.458e-03,  LearnRate: 1.000e-01, Time taken : 5.308s\n",
      "\tEpoch 15, Step 0100, MSE: 8.817e-03, Ground truth MSE: 9.271e-03,  LearnRate: 1.000e-01, Time taken : 5.238s\n",
      "\tEpoch 15, Step 0150, MSE: 8.810e-03, Ground truth MSE: 9.246e-03,  LearnRate: 1.000e-01, Time taken : 5.668s\n",
      "Epoch 15 time: 21.061s, average error = 93.860mrad\n",
      "Starting Epoch 16, with 156 batches\n",
      "\tEpoch 16, Step 0000, MSE: 8.943e-03, Ground truth MSE: 8.868e-03,  LearnRate: 1.000e-01, Time taken : 5.301s\n",
      "\tEpoch 16, Step 0050, MSE: 8.786e-03, Ground truth MSE: 8.803e-03,  LearnRate: 1.000e-01, Time taken : 4.821s\n",
      "\tEpoch 16, Step 0100, MSE: 8.766e-03, Ground truth MSE: 8.765e-03,  LearnRate: 1.000e-01, Time taken : 5.212s\n",
      "\tEpoch 16, Step 0150, MSE: 8.802e-03, Ground truth MSE: 8.831e-03,  LearnRate: 1.000e-01, Time taken : 5.056s\n",
      "Epoch 16 time: 20.088s, average error = 93.818mrad\n",
      "Starting Epoch 17, with 156 batches\n",
      "\tEpoch 17, Step 0000, MSE: 8.943e-03, Ground truth MSE: 9.221e-03,  LearnRate: 1.000e-01, Time taken : 5.329s\n",
      "\tEpoch 17, Step 0050, MSE: 9.299e-03, Ground truth MSE: 9.722e-03,  LearnRate: 1.000e-01, Time taken : 5.422s\n",
      "\tEpoch 17, Step 0100, MSE: 8.879e-03, Ground truth MSE: 9.182e-03,  LearnRate: 1.000e-01, Time taken : 5.380s\n",
      "\tEpoch 17, Step 0150, MSE: 8.790e-03, Ground truth MSE: 9.164e-03,  LearnRate: 1.000e-01, Time taken : 5.187s\n",
      "Epoch 17 time: 20.843s, average error = 93.755mrad\n",
      "Starting Epoch 18, with 156 batches\n",
      "\tEpoch 18, Step 0000, MSE: 8.726e-03, Ground truth MSE: 8.952e-03,  LearnRate: 1.000e-01, Time taken : 5.156s\n",
      "\tEpoch 18, Step 0050, MSE: 8.862e-03, Ground truth MSE: 9.119e-03,  LearnRate: 1.000e-01, Time taken : 4.908s\n",
      "\tEpoch 18, Step 0100, MSE: 8.996e-03, Ground truth MSE: 9.232e-03,  LearnRate: 1.000e-01, Time taken : 5.017s\n",
      "\tEpoch 18, Step 0150, MSE: 8.727e-03, Ground truth MSE: 8.975e-03,  LearnRate: 1.000e-01, Time taken : 5.350s\n",
      "Epoch 18 time: 20.247s, average error = 93.420mrad\n",
      "Starting Epoch 19, with 156 batches\n",
      "\tEpoch 19, Step 0000, MSE: 8.808e-03, Ground truth MSE: 9.026e-03,  LearnRate: 1.000e-01, Time taken : 5.341s\n",
      "\tEpoch 19, Step 0050, MSE: 8.787e-03, Ground truth MSE: 8.949e-03,  LearnRate: 1.000e-01, Time taken : 5.116s\n",
      "\tEpoch 19, Step 0100, MSE: 8.839e-03, Ground truth MSE: 9.056e-03,  LearnRate: 1.000e-01, Time taken : 5.215s\n",
      "\tEpoch 19, Step 0150, MSE: 8.849e-03, Ground truth MSE: 9.015e-03,  LearnRate: 1.000e-01, Time taken : 5.270s\n",
      "Epoch 19 time: 20.580s, average error = 94.069mrad\n",
      "Starting Epoch 20, with 156 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dad7ab1fef9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai_kup_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mai_kup_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_model_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms_HB.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfname_data_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model_ground_truth_campaign4_1000ms_HB.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-64ea29b26113>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(acc, phase, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     16\u001b[0m                             \u001b[0mx_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmccv_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_fraction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                             \u001b[0mepoch_learning_rate_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_learning_rate_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msgd_switch_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                             prob_keep = 0.6,ground_truth_frac=0.2, ground_truth_periods = 7,fname_model_out=fname_model_out,fname_data_out=fname_data_out)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-ef5ee2cd0490>\u001b[0m in \u001b[0;36mtrain_RBM_complete_mccv\u001b[0;34m(input_size, output_size, hidden_sizes, x_data, y_data, batch_size, epochs, test_fraction, test_interval, initial_learning_rate, epoch_learning_rate_decay, epoch_learning_rate_interval, sgd_switch_epoch, prob_keep, ground_truth_frac, ground_truth_periods, fname_model_out, fname_data_out)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0;31m# Check how our model performs against the ground truth test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_mse_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mground_truthtest_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mcurr_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/intel_3.6_TF_1.9/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(ai_kup_acc,ai_kup_phase,fname_model_out='./model_ground_truth_campaign4a_100ms_HB.ckpt',fname_data_out='./model_ground_truth_campaign4a_100ms_HB.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
