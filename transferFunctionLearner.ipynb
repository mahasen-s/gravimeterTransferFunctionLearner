{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float32\n",
    "DTYPE_np = np.float32\n",
    "log_dir = '/tmp/funclearn' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = get_biases([output_dim],DTYPE)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n",
    "def train_RBM_complete(input_size=1,output_size=1,hidden_size=1,num_layers=5,model_func=None,unknown_func=None,\n",
    "                  test_x=None, test_y = None, generate_training_samples_fun=None,\n",
    "                  batch_size=25, max_steps = 1000, epochs =5,\n",
    "                  initial_learning_rate = 0.02, decay_rate = 1/math.e, decay_steps=1000,\n",
    "                  prob_keep = 0.8):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_complete(x,input_size,output_size,hidden_size,is_test,global_step,1,num_layers=num_layers)\n",
    "\n",
    "        # Should add dropout!\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            for i in range(max_steps):\n",
    "                if i % 100 ==0:\n",
    "                    # Check how our model performs against the test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('Epoch %d, Step %04d, MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = generate_training_samples_fun(epoch_ind, global_step)\n",
    "                    summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            sess.run(increment_epoch_ind)\n",
    "        train_writer.close()\n",
    "        test_writer.close()     \n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex nonlinear function\n",
    "Example of learning a toy random nonlinear compelex-valued function. \n",
    "We separate real and imaginary parts and concatenate them into the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to create a random linear operator, f, \n",
    "# and another function to generate (x, f(x)) pairs\n",
    "\n",
    "def build_unknown_complex_nonlinear_func(m,n):\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    \n",
    "    #mat = tf.constant(np.random.rand(m,n),dtype=DTYPE)\n",
    "    #def unknown_func(x):\n",
    "    #   return tf.tensordot(x,mat,1) # check!\n",
    "    \n",
    "    mat0 = np.random.rand(m,n).astype(DTYPE_np) + 1j*np.random.rand(m,n).astype(DTYPE_np)\n",
    "    mat1 = np.random.rand(m,n).astype(DTYPE_np) + 1j*np.random.rand(m,n).astype(DTYPE_np)\n",
    "    \n",
    "    def unknown_func(x):\n",
    "        return np.matmul(np.power(x,2),mat0) + np.matmul(x,mat1)\n",
    "    \n",
    "    return [(mat0,mat1),unknown_func]\n",
    "\n",
    "def generate_samples(m,batch_size,unknown_func):\n",
    "    # Let's just assume that the input x values are random\n",
    "    \n",
    "    x = np.random.rand(batch_size,m)\n",
    "    y = unknown_func(x) \n",
    "    \n",
    "    # Concat real and imag\n",
    "    x_cat = np.concatenate((np.real(x),np.imag(x)),axis=1)\n",
    "    y_cat = np.concatenate((np.real(y),np.imag(y)),axis=1)\n",
    "    \n",
    "    return [x_cat,y_cat]\n",
    "\n",
    "def train_complex_func():\n",
    "    input_size = 8\n",
    "    output_size = 13\n",
    "    \n",
    "    hidden_size = 30\n",
    "    num_layers = 5\n",
    "    batch_size = 25\n",
    "    \n",
    "    test_size = 200\n",
    "    max_steps = 2001\n",
    "    decay_steps = 500\n",
    "    \n",
    "    \n",
    "    mat,unknown_func = build_unknown_complex_nonlinear_func(input_size,output_size)\n",
    "    test_x, test_y = generate_samples(input_size,test_size,unknown_func)\n",
    "    \n",
    "    # Define the function that will be called to create a new set batch of training data.\n",
    "    # It will be passed two arguments: epoch and global step (which is NOT reset at the start of each epoch)\n",
    "    generate_training_samples_fun = lambda epoch_ind, global_step: generate_samples(input_size,batch_size,unknown_func) # \n",
    "    \n",
    "    train_RBM_complete(input_size=2*input_size,output_size=2*output_size,hidden_size=hidden_size,num_layers = num_layers,unknown_func=unknown_func,\n",
    "                  test_x=test_x, test_y = test_y, generate_training_samples_fun=generate_training_samples_fun,\n",
    "                  batch_size=25, max_steps = max_steps, epochs =5, initial_learning_rate = 0.02, decay_rate = 1/math.e, decay_steps=decay_steps, prob_keep = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0000, MSE: 4.233e+01,  LearnRate: 2.000e-02, Time taken : 0.323s\n",
      "Epoch 0, Step 0100, MSE: 3.496e-01,  LearnRate: 2.000e-02, Time taken : 2.049s\n",
      "Epoch 0, Step 0200, MSE: 1.809e-01,  LearnRate: 2.000e-02, Time taken : 0.879s\n",
      "Epoch 0, Step 0300, MSE: 1.076e-01,  LearnRate: 2.000e-02, Time taken : 0.802s\n",
      "Epoch 0, Step 0400, MSE: 9.051e-02,  LearnRate: 2.000e-02, Time taken : 0.848s\n",
      "Epoch 0, Step 0500, MSE: 9.097e-02,  LearnRate: 2.000e-02, Time taken : 0.990s\n",
      "Epoch 0, Step 0600, MSE: 7.832e-02,  LearnRate: 7.358e-03, Time taken : 0.945s\n",
      "Epoch 0, Step 0700, MSE: 7.033e-02,  LearnRate: 7.358e-03, Time taken : 0.878s\n",
      "Epoch 0, Step 0800, MSE: 7.854e-02,  LearnRate: 7.358e-03, Time taken : 0.831s\n",
      "Epoch 0, Step 0900, MSE: 7.081e-02,  LearnRate: 7.358e-03, Time taken : 0.871s\n",
      "Epoch 0, Step 1000, MSE: 6.770e-02,  LearnRate: 7.358e-03, Time taken : 0.941s\n",
      "Epoch 0, Step 1100, MSE: 5.948e-02,  LearnRate: 2.707e-03, Time taken : 0.974s\n",
      "Epoch 0, Step 1200, MSE: 5.801e-02,  LearnRate: 2.707e-03, Time taken : 0.991s\n",
      "Epoch 0, Step 1300, MSE: 5.778e-02,  LearnRate: 2.707e-03, Time taken : 0.908s\n",
      "Epoch 0, Step 1400, MSE: 5.530e-02,  LearnRate: 2.707e-03, Time taken : 0.901s\n",
      "Epoch 0, Step 1500, MSE: 5.497e-02,  LearnRate: 2.707e-03, Time taken : 0.967s\n",
      "Epoch 0, Step 1600, MSE: 5.435e-02,  LearnRate: 9.957e-04, Time taken : 1.027s\n",
      "Epoch 0, Step 1700, MSE: 5.360e-02,  LearnRate: 9.957e-04, Time taken : 0.931s\n",
      "Epoch 0, Step 1800, MSE: 5.281e-02,  LearnRate: 9.957e-04, Time taken : 0.924s\n",
      "Epoch 0, Step 1900, MSE: 5.270e-02,  LearnRate: 9.957e-04, Time taken : 0.897s\n",
      "Epoch 0, Step 2000, MSE: 5.207e-02,  LearnRate: 9.957e-04, Time taken : 0.896s\n",
      "Epoch 1, Step 0000, MSE: 5.207e-02,  LearnRate: 5.437e-02, Time taken : 0.032s\n",
      "Epoch 1, Step 0100, MSE: 4.768e-02,  LearnRate: 2.000e-02, Time taken : 0.941s\n",
      "Epoch 1, Step 0200, MSE: 5.149e-02,  LearnRate: 2.000e-02, Time taken : 0.896s\n",
      "Epoch 1, Step 0300, MSE: 4.327e-02,  LearnRate: 2.000e-02, Time taken : 0.869s\n",
      "Epoch 1, Step 0400, MSE: 4.937e-02,  LearnRate: 2.000e-02, Time taken : 0.846s\n",
      "Epoch 1, Step 0500, MSE: 3.159e-02,  LearnRate: 2.000e-02, Time taken : 0.837s\n",
      "Epoch 1, Step 0600, MSE: 4.086e-02,  LearnRate: 7.358e-03, Time taken : 0.975s\n",
      "Epoch 1, Step 0700, MSE: 4.278e-02,  LearnRate: 7.358e-03, Time taken : 0.934s\n",
      "Epoch 1, Step 0800, MSE: 2.937e-02,  LearnRate: 7.358e-03, Time taken : 0.931s\n",
      "Epoch 1, Step 0900, MSE: 2.747e-02,  LearnRate: 7.358e-03, Time taken : 0.916s\n",
      "Epoch 1, Step 1000, MSE: 2.884e-02,  LearnRate: 7.358e-03, Time taken : 0.925s\n",
      "Epoch 1, Step 1100, MSE: 2.444e-02,  LearnRate: 2.707e-03, Time taken : 0.892s\n",
      "Epoch 1, Step 1200, MSE: 2.679e-02,  LearnRate: 2.707e-03, Time taken : 0.866s\n",
      "Epoch 1, Step 1300, MSE: 2.667e-02,  LearnRate: 2.707e-03, Time taken : 0.909s\n",
      "Epoch 1, Step 1400, MSE: 2.592e-02,  LearnRate: 2.707e-03, Time taken : 0.935s\n",
      "Epoch 1, Step 1500, MSE: 2.762e-02,  LearnRate: 2.707e-03, Time taken : 0.881s\n",
      "Epoch 1, Step 1600, MSE: 2.176e-02,  LearnRate: 9.957e-04, Time taken : 0.941s\n",
      "Epoch 1, Step 1700, MSE: 2.112e-02,  LearnRate: 9.957e-04, Time taken : 0.942s\n",
      "Epoch 1, Step 1800, MSE: 2.162e-02,  LearnRate: 9.957e-04, Time taken : 0.912s\n",
      "Epoch 1, Step 1900, MSE: 2.051e-02,  LearnRate: 9.957e-04, Time taken : 0.888s\n",
      "Epoch 1, Step 2000, MSE: 2.088e-02,  LearnRate: 9.957e-04, Time taken : 0.958s\n",
      "Epoch 2, Step 0000, MSE: 2.088e-02,  LearnRate: 5.437e-02, Time taken : 0.008s\n",
      "Epoch 2, Step 0100, MSE: 3.217e-02,  LearnRate: 2.000e-02, Time taken : 0.848s\n",
      "Epoch 2, Step 0200, MSE: 6.804e-02,  LearnRate: 2.000e-02, Time taken : 0.980s\n",
      "Epoch 2, Step 0300, MSE: 2.428e-02,  LearnRate: 2.000e-02, Time taken : 0.944s\n",
      "Epoch 2, Step 0400, MSE: 3.071e-02,  LearnRate: 2.000e-02, Time taken : 0.824s\n",
      "Epoch 2, Step 0500, MSE: 1.398e-01,  LearnRate: 2.000e-02, Time taken : 0.919s\n",
      "Epoch 2, Step 0600, MSE: 2.220e-02,  LearnRate: 7.358e-03, Time taken : 0.996s\n",
      "Epoch 2, Step 0700, MSE: 1.844e-02,  LearnRate: 7.358e-03, Time taken : 0.959s\n",
      "Epoch 2, Step 0800, MSE: 2.455e-02,  LearnRate: 7.358e-03, Time taken : 1.039s\n",
      "Epoch 2, Step 0900, MSE: 1.361e-02,  LearnRate: 7.358e-03, Time taken : 0.993s\n",
      "Epoch 2, Step 1000, MSE: 3.975e-02,  LearnRate: 7.358e-03, Time taken : 1.075s\n",
      "Epoch 2, Step 1100, MSE: 1.464e-02,  LearnRate: 2.707e-03, Time taken : 0.956s\n",
      "Epoch 2, Step 1200, MSE: 1.005e-02,  LearnRate: 2.707e-03, Time taken : 0.927s\n",
      "Epoch 2, Step 1300, MSE: 1.020e-02,  LearnRate: 2.707e-03, Time taken : 0.849s\n",
      "Epoch 2, Step 1400, MSE: 1.054e-02,  LearnRate: 2.707e-03, Time taken : 0.845s\n",
      "Epoch 2, Step 1500, MSE: 1.054e-02,  LearnRate: 2.707e-03, Time taken : 0.872s\n",
      "Epoch 2, Step 1600, MSE: 1.174e-02,  LearnRate: 9.957e-04, Time taken : 0.860s\n",
      "Epoch 2, Step 1700, MSE: 1.510e-02,  LearnRate: 9.957e-04, Time taken : 0.873s\n",
      "Epoch 2, Step 1800, MSE: 8.796e-03,  LearnRate: 9.957e-04, Time taken : 0.862s\n",
      "Epoch 2, Step 1900, MSE: 9.229e-03,  LearnRate: 9.957e-04, Time taken : 0.803s\n",
      "Epoch 2, Step 2000, MSE: 8.106e-03,  LearnRate: 9.957e-04, Time taken : 0.791s\n",
      "Epoch 3, Step 0000, MSE: 8.106e-03,  LearnRate: 5.437e-02, Time taken : 0.009s\n",
      "Epoch 3, Step 0100, MSE: 3.103e-02,  LearnRate: 2.000e-02, Time taken : 0.811s\n",
      "Epoch 3, Step 0200, MSE: 2.815e-02,  LearnRate: 2.000e-02, Time taken : 0.880s\n",
      "Epoch 3, Step 0300, MSE: 1.892e-02,  LearnRate: 2.000e-02, Time taken : 0.899s\n",
      "Epoch 3, Step 0400, MSE: 1.374e-02,  LearnRate: 2.000e-02, Time taken : 0.891s\n",
      "Epoch 3, Step 0500, MSE: 3.689e-02,  LearnRate: 2.000e-02, Time taken : 0.858s\n",
      "Epoch 3, Step 0600, MSE: 6.492e-03,  LearnRate: 7.358e-03, Time taken : 0.887s\n",
      "Epoch 3, Step 0700, MSE: 6.723e-03,  LearnRate: 7.358e-03, Time taken : 0.852s\n",
      "Epoch 3, Step 0800, MSE: 9.801e-03,  LearnRate: 7.358e-03, Time taken : 0.895s\n",
      "Epoch 3, Step 0900, MSE: 8.107e-03,  LearnRate: 7.358e-03, Time taken : 0.869s\n",
      "Epoch 3, Step 1000, MSE: 8.188e-03,  LearnRate: 7.358e-03, Time taken : 0.847s\n",
      "Epoch 3, Step 1100, MSE: 8.410e-03,  LearnRate: 2.707e-03, Time taken : 0.843s\n",
      "Epoch 3, Step 1200, MSE: 4.602e-03,  LearnRate: 2.707e-03, Time taken : 0.895s\n",
      "Epoch 3, Step 1300, MSE: 6.540e-03,  LearnRate: 2.707e-03, Time taken : 0.956s\n",
      "Epoch 3, Step 1400, MSE: 5.096e-03,  LearnRate: 2.707e-03, Time taken : 0.849s\n",
      "Epoch 3, Step 1500, MSE: 8.566e-03,  LearnRate: 2.707e-03, Time taken : 0.836s\n",
      "Epoch 3, Step 1600, MSE: 4.987e-03,  LearnRate: 9.957e-04, Time taken : 0.961s\n",
      "Epoch 3, Step 1700, MSE: 5.063e-03,  LearnRate: 9.957e-04, Time taken : 0.914s\n",
      "Epoch 3, Step 1800, MSE: 5.553e-03,  LearnRate: 9.957e-04, Time taken : 0.848s\n",
      "Epoch 3, Step 1900, MSE: 4.458e-03,  LearnRate: 9.957e-04, Time taken : 0.875s\n",
      "Epoch 3, Step 2000, MSE: 9.627e-03,  LearnRate: 9.957e-04, Time taken : 0.883s\n",
      "Epoch 4, Step 0000, MSE: 9.627e-03,  LearnRate: 5.437e-02, Time taken : 0.007s\n",
      "Epoch 4, Step 0100, MSE: 3.662e-02,  LearnRate: 2.000e-02, Time taken : 0.874s\n",
      "Epoch 4, Step 0200, MSE: 2.133e-02,  LearnRate: 2.000e-02, Time taken : 0.864s\n",
      "Epoch 4, Step 0300, MSE: 1.693e-02,  LearnRate: 2.000e-02, Time taken : 0.885s\n",
      "Epoch 4, Step 0400, MSE: 1.312e-02,  LearnRate: 2.000e-02, Time taken : 0.888s\n",
      "Epoch 4, Step 0500, MSE: 1.722e-02,  LearnRate: 2.000e-02, Time taken : 0.888s\n",
      "Epoch 4, Step 0600, MSE: 8.178e-03,  LearnRate: 7.358e-03, Time taken : 0.928s\n",
      "Epoch 4, Step 0700, MSE: 1.864e-02,  LearnRate: 7.358e-03, Time taken : 0.936s\n",
      "Epoch 4, Step 0800, MSE: 7.340e-03,  LearnRate: 7.358e-03, Time taken : 0.899s\n",
      "Epoch 4, Step 0900, MSE: 1.036e-02,  LearnRate: 7.358e-03, Time taken : 0.873s\n",
      "Epoch 4, Step 1000, MSE: 2.931e-02,  LearnRate: 7.358e-03, Time taken : 0.862s\n",
      "Epoch 4, Step 1100, MSE: 6.394e-03,  LearnRate: 2.707e-03, Time taken : 0.876s\n",
      "Epoch 4, Step 1200, MSE: 5.402e-03,  LearnRate: 2.707e-03, Time taken : 0.936s\n",
      "Epoch 4, Step 1300, MSE: 4.610e-03,  LearnRate: 2.707e-03, Time taken : 0.842s\n",
      "Epoch 4, Step 1400, MSE: 7.503e-03,  LearnRate: 2.707e-03, Time taken : 0.828s\n",
      "Epoch 4, Step 1500, MSE: 5.053e-03,  LearnRate: 2.707e-03, Time taken : 0.916s\n",
      "Epoch 4, Step 1600, MSE: 9.811e-03,  LearnRate: 9.957e-04, Time taken : 0.942s\n",
      "Epoch 4, Step 1700, MSE: 4.044e-03,  LearnRate: 9.957e-04, Time taken : 0.933s\n",
      "Epoch 4, Step 1800, MSE: 4.170e-03,  LearnRate: 9.957e-04, Time taken : 0.851s\n",
      "Epoch 4, Step 1900, MSE: 3.968e-03,  LearnRate: 9.957e-04, Time taken : 0.840s\n",
      "Epoch 4, Step 2000, MSE: 4.531e-03,  LearnRate: 9.957e-04, Time taken : 0.935s\n",
      "\n",
      "Total time:\t 91.587s\n"
     ]
    }
   ],
   "source": [
    "train_complex_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './ChrisFreierPhDCampaigns/campaign4a.h5'\n",
    "f = h5py.File(fname,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(<HDF5 file \"campaign4a.h5\" (mode r)>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: %s\" % f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(T,tau)\n",
      "accelerometer\n",
      "ai_kdown\n",
      "ai_kup\n"
     ]
    }
   ],
   "source": [
    "for key in f.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerometer = np.array(f['accelerometer'])\n",
    "ai_kdown = np.array(f['ai_kdown'])\n",
    "ai_kup = np.array(f['ai_kup'])\n",
    "T_tau = np.array(f['(T,tau)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000001, 2)\n",
      "(103427, 2)\n",
      "(103403, 2)\n"
     ]
    }
   ],
   "source": [
    "print(accelerometer.shape)\n",
    "print(ai_kdown.shape)\n",
    "print(ai_kup.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(T,tau)', 'accelerometer', 'ai_kdown', 'ai_kup']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(T,tau)', <HDF5 dataset \"(T,tau)\": shape (2,), type \"<f8\">),\n",
       " ('accelerometer',\n",
       "  <HDF5 dataset \"accelerometer\": shape (100000001, 2), type \"<f8\">),\n",
       " ('ai_kdown', <HDF5 dataset \"ai_kdown\": shape (103427, 2), type \"<f8\">),\n",
       " ('ai_kup', <HDF5 dataset \"ai_kup\": shape (103403, 2), type \"<f8\">)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(f.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(T,tau)', <HDF5 dataset \"(T,tau)\": shape (2,), type \"<f8\">),\n",
       " ('accelerometer',\n",
       "  <HDF5 dataset \"accelerometer\": shape (100000001, 2), type \"<f8\">),\n",
       " ('ai_kdown', <HDF5 dataset \"ai_kdown\": shape (80337, 2), type \"<f8\">),\n",
       " ('ai_kup', <HDF5 dataset \"ai_kup\": shape (80347, 2), type \"<f8\">)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = './ChrisFreierPhDCampaigns/campaign5a.h5'\n",
    "f0 = h5py.File(fname,'r')\n",
    "list(f0.keys())\n",
    "list(f0.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel_3.6_TF_1.8]",
   "language": "python",
   "name": "conda-env-intel_3.6_TF_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
