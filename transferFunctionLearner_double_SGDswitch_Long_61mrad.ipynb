{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning prototype model\n",
    "Tensorflow implementation of a fully connected deep net for arbitrarily shaped input and output layers, with the following features:\n",
    "- Easy control of net topology and hyperparameters\n",
    "- Abstracted functions for retrieving/constructing training data\n",
    "- Exponential decay of learning rate\n",
    "- Dropout regularisation\n",
    "- Batch normalisation\n",
    "- Automatic logging of bulk statistics of each layer through tensorboard (and convenient functions to attach logging to custom functions)\n",
    "\n",
    "\n",
    "## Visualising progress\n",
    "To inspect the training progress, run\n",
    "\n",
    ">  `>tensorboard --logdir=<log_dir>`\n",
    "\n",
    "\n",
    "from the terminal, where `log_dir` as as defined above\n",
    "\n",
    "## Requirements\n",
    "- TensorFlow 1.8\n",
    "- numpy\n",
    "- subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import time, math\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting dtype for tensorflow and numpy environments\n",
    "DTYPE = tf.float64\n",
    "DTYPE_np = np.float64\n",
    "log_dir = '/mnt/dataDrive3/mahasen/tmp/funclearn' # Directory where we dump tensorboard log files\n",
    "\n",
    "# Useful function for resetting logdir and tensorflow graph\n",
    "def reset_tf():\n",
    "    call([\"rm\",\"-rf\",log_dir+'/'])\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "# Define some functions for initialising layers with appropriate statistics    \n",
    "def get_weights(shape,dtype):\n",
    "    # Returns trainable weight variable, initialised from truncated (+\\- 2std. dev. only) standard normal distribution\n",
    "    return tf.Variable(tf.truncated_normal(shape, dtype=dtype),name='weights')\n",
    "\n",
    "def get_biases(shape,dtype):\n",
    "    # Returns trainable bias variable, initialised arbitrarily as a small constant\n",
    "    return tf.Variable(tf.constant(0.01, shape=shape, dtype=dtype),name='biases')\n",
    "\n",
    "def get_bn_offset(shape,dtype):\n",
    "    # Returns trainable bias/offset variable for batch normalisation\n",
    "    return tf.Variable(tf.truncated_normal(shape=shape, dtype=dtype),name='beta_offset')\n",
    "\n",
    "def get_bn_scale(shape,dtype):\n",
    "    # Returns trainable scale variable for batch normalisation\n",
    "    return tf.Variable(tf.constant(1.0, shape=shape, dtype=dtype),name='gamma_scale')\n",
    "\n",
    "\n",
    "def variable_summaries(var):\n",
    "    # Attaches mean, stddev, max, min, and a histogram of an input var to a tensor\n",
    "    # Useful for TensorBoard visualisation\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var,name='mean')\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)),name='stddev')\n",
    "        \n",
    "        tf.summary.scalar('mean',mean)\n",
    "        tf.summary.scalar('stddev',stddev)\n",
    "        tf.summary.scalar('max',tf.reduce_max(var))\n",
    "        tf.summary.scalar('min',tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "def batchnorm(logits, is_test, offset, scale, iteration):\n",
    "    # Get summary statistics of this batch\n",
    "    mean, variance    = tf.nn.moments(logits, [0],name='moments')\n",
    "    \n",
    "    # We'll use an exponential moving average over the training iterations during test time\n",
    "    # This is a tool to do that\n",
    "    exp_moving_avg    = tf.train.ExponentialMovingAverage(0.9999, iteration)\n",
    "    update_moving_avg = exp_moving_avg.apply([mean, variance])\n",
    "    \n",
    "    # If this is the test, we use the m,v values we obtained from the exponential moving average \n",
    "    # over mean, variance that we obtained from training. otherwise use the batch mean, variance\n",
    "    mean_cond        = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean, name='mean_cond')\n",
    "    variance_cond    = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance, name='variance_cond')\n",
    "    \n",
    "    # This applies the following normalisation: x-> scale*(x-mean(x))/(variance_epsilon+std(x)) + offset\n",
    "    logits_bn = tf.nn.batch_normalization(logits, mean_cond, variance_cond, offset, scale, variance_epsilon=1e-5,name='logits_batchnormed')\n",
    "    \n",
    "    return logits_bn, update_moving_avg\n",
    "\n",
    "def get_layer_complete(input_tensor,input_dim, output_dim, layer_name, is_test, prob_keep, global_step, act_func=tf.nn.relu):\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            weights = get_weights([input_dim, output_dim],DTYPE)\n",
    "            variable_summaries(weights)\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = get_biases([output_dim],DTYPE)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('batchnorm'):\n",
    "            offset = get_bn_offset([output_dim], DTYPE)\n",
    "            scale  = get_bn_scale([output_dim], DTYPE)\n",
    "        \n",
    "        logits = tf.add(tf.matmul(input_tensor, weights),biases,name='logits')\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        logits_bn, update_moving_avg = batchnorm(logits, is_test, offset, scale, global_step)\n",
    "        tf.summary.histogram('logits_batchNormed', logits_bn)\n",
    "        activated = act_func(logits_bn, name='activation')\n",
    "        dropped_out = tf.nn.dropout(activated,prob_keep,name='dropout')\n",
    "        tf.summary.histogram('activations', activated)\n",
    "        return dropped_out, update_moving_avg \n",
    "    \n",
    "def func_deep_learner_complete(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu,num_layers=5):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is the number of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h,'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h,h,'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h,n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates\n",
    "\n",
    "def train_RBM_complete(input_size=1,output_size=1,hidden_size=1,num_layers=5,model_func=None,unknown_func=None,\n",
    "                  test_x=None, test_y = None, generate_training_samples_fun=None,\n",
    "                  batch_size=25, max_steps = 1000, epochs =5,\n",
    "                  initial_learning_rate = 0.02, decay_rate = 1/math.e, decay_steps=1000,\n",
    "                  prob_keep = 0.8):\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_complete(x,input_size,output_size,hidden_size,is_test,global_step,1,num_layers=num_layers)\n",
    "\n",
    "        # Should add dropout!\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            for i in range(max_steps):\n",
    "                if i % 100 ==0:\n",
    "                    # Check how our model performs against the test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('Epoch %d, Step %04d, MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = generate_training_samples_fun(epoch_ind, global_step)\n",
    "                    summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            sess.run(increment_epoch_ind)\n",
    "        train_writer.close()\n",
    "        test_writer.close()     \n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a Mach-Zehnder interferometer with interrogation time $T$ and pulse duration $\\tau$. The total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\phi_{eff}^1 - 2\\phi_{eff}^2 + \\phi_{eff}^3 + \\left(\\mathrm{arg}(\\Theta_0^1)-\\mathrm{arg}(\\Theta^3_0)\\right)$$\n",
    "where $\\phi_{eff}^i$ describes the \"light phase at the atomic positions during the three Raman pulses\". $\\Theta_0^i$ descibes secondary phase shifts due to \"different light shifts between the first and last pulse\".\n",
    "\n",
    "Define the sensitivity function $g(t)$ as the effect on the total interferometer phase due to a phase jump $\\delta\\phi$ at time $t$. For the three-pulse Mach-Zehnder, with second pulse centered at $t=0$, $g(t)$ is given by:\n",
    "$$g(t) = \\begin{cases}\n",
    "\\sin(\\Omega_rt), & 0<t\\leq\\tau \\\\\n",
    "1, & \\tau<t\\leq T+\\tau\\\\\n",
    "-\\sin(\\Omega_r(T-t)), & T+\\tau<t\\leq T+2\\tau \\\\\n",
    "0, & t>T+2\\tau\n",
    "\\end{cases}$$\n",
    "\n",
    "For finite Raman pulse duration (i.e. $\\tau>0$), the total interferometer phase is given by:\n",
    "$$\\Delta\\Phi = \\int_{-(T_2\\tau)}^{(T+2\\tau)}g(t)\\frac{d\\phi(t)}{dt}dt$$\n",
    "\n",
    "To perform post-correction, we separtely calculate the phase offset caused by mirror vibrations:\n",
    "$$\\Phi_{vib} = k_{eff}\\int_{t_1}^{t_3}g(t)v(t)dt$$\n",
    "where $v(t)=\\frac{1}{k_{eff}}\\frac{d\\phi}{dt}$ is the mirror velocity and $k_eff$ is the effective wavevector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "Data is organised into campaigns, each containing ~10^5 interferometer runs. Use only 4a onwards.\n",
    "Each `.h5` has three fields\n",
    "- `accelerometer`: `[N_a,2]` linearly proportional to the raw out from the accelerometer as a time series. 1st column is linux time, 2nd column is signal\n",
    "- `ai_kdown`: `[N_p,2]` total interferometer phase for kdown? configuration. 1st column is linux time, 2nd column is phase. Each row is the total interferometer phase for an Mach-Zehnder sequence with 2nd pulse centered at the given time\n",
    "- `ai_kup`: `[N_p,2]` total interferometer phase for kup? configuration. Same layout as `ai_kdown`\n",
    "- `(T,tau)`: `[2,]` the interferometer interrogation time `T` and the $\\frac{\\pi}{2}$ Raman pulse duration `tau`\n",
    "\n",
    "First we need to prepare the data.\n",
    "\n",
    "(1) Check timestamps of `ai_kdown`==`ai_kup`\n",
    "- THEY DON'T; they aren't even necessarily the same length. Assume that each corresponds to a completely different subsequence of `accelerometer`\n",
    "- Also, note that the distribution of delays between successive runs is weirdly distributed\n",
    "\n",
    "(2) For each ai_* associate a contiguous subsequence of `accelerometer`\n",
    " - Ensure each subsequence is of equal length `N_s`\n",
    " - Will being left/right aligned w.r.t. rounding of `ai_kdown` timestamp to `accelerometer` timestamps affect anything?\n",
    " - Safest thing to do might be linearly interpolate `accelerometer` to ensure that `accelerometer` subsequence is correctly time-aligned with `ai_*`\n",
    " \n",
    "(3) Construct our inputs and outputs\n",
    " - `x_input` = `[N_s,N_p]` array\n",
    " - `y_output` = `[N_p,2]` concatenate 2nd column of `ai_kdown` and `ai_kup`\n",
    "    \n",
    "## Check\n",
    "Make sure we can reproduce original data (i.e. reproduce original $\\Phi_{vib}$)\n",
    " - Do we have this data?\n",
    "    \n",
    "## Modelling\n",
    "(1) *Model-free* Just use the inputs and labels as they are (assuming `N_s` is small enough)\n",
    " - Pros: easy\n",
    " - Cons: no ground-truth to compare against\n",
    "\n",
    "(2) *Explicit transfer function* Apply unknown function in Laplace space ($\\mathcal{L}\\{y(t)\\}(Z) =H(Z)\\mathcal{L}\\{x(t)\\}(Z)$ )\n",
    " - Pros: ground-truth to compare against\n",
    " - Cons: less easy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Processed campaign4a\n",
    "Matlab used to assign subsequences in `accelerometer` to each entry in `ai_kup` and `ai_kdown`.\n",
    "Subsequence length is 195. ~100000 entries for each of `ai_kup` and `ai_kdown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './campaign4a_proc.h5'\n",
    "f0 = h5py.File(fname,'r')\n",
    "list(f0.keys())\n",
    "list(f0.items())\n",
    "dset = f0['/ai_kup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function that will generate indices for batches\n",
    "# accounting for the possibility that the size of the dataset\n",
    "# will not be an even multiple of the batch size\n",
    "def generate_batches(N,batch_size=32):\n",
    "    # N is the number of elements in the dataset\n",
    "    if N<batch_size:\n",
    "        raise ValueError('batch_size must be smaller than N')\n",
    "    perm = np.random.permutation(N);\n",
    "    if np.mod(N,batch_size)!=0:\n",
    "        # We need to append to perm so that is is an even multiple of batch_size\n",
    "        perm2= np.random.permutation(N);\n",
    "        perm = np.concatenate((perm,perm2[0:batch_size-np.mod(N,batch_size)]))\n",
    "    \n",
    "    n_batches = np.int(len(perm)/batch_size)\n",
    "    batches = perm.reshape(batch_size,n_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def get_xy_by_inds(x,y,inds):\n",
    "    # Get values from 2Dx, 1Dy arrays\n",
    "    x0 = x[inds,:]\n",
    "    y0 = np.array(list(map(y.__getitem__,inds))) # because Python treats 1D arrays differently from ND arrays\n",
    "    y0 = y0.reshape([len(y0),1])\n",
    "    return x0,y0\n",
    "    \n",
    "def generate_testtrain(x,y,test_fraction,batch_size=32):\n",
    "    # Assumes x.shape = [n_samples,input_size], y.shape = [input_size,]\n",
    "    if test_fraction<0 or test_fraction>1:\n",
    "        raise ValueError('test_fraction must be between 0 and 1')\n",
    "    \n",
    "    n_samples  = x.shape[0]\n",
    "    test_size  = np.int(np.round(n_samples*test_fraction))\n",
    "    if test_size<1:\n",
    "        raise ValueError('test_fraction is too small')\n",
    "    train_size = np.int(n_samples-test_size)\n",
    "    \n",
    "    # Permute x,y\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    permi= np.argsort(perm)\n",
    "    x0,y0= get_xy_by_inds(x,y,permi)\n",
    "                    \n",
    "    train_x = x0[:train_size,:]\n",
    "    train_y = y0[:train_size,:]\n",
    "    \n",
    "    test_x  = x0[train_size:,:]\n",
    "    test_y  = y0[train_size:,:]\n",
    "    \n",
    "    batches = generate_batches(train_size,batch_size)\n",
    "    \n",
    "    return test_x,test_y,train_x,train_y,batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_deep_learner_arbshape(x,m,n,h,is_test,global_step,prob_keep,act_func=tf.nn.relu):\n",
    "    # x is the input variable, probably a TensorFlow placeholder object\n",
    "    # m is the input dimension\n",
    "    # n is the output dimension\n",
    "    # h is a LIST of hidden neurons\n",
    "    # act_func is the activation function to be applied\n",
    "    # is_test is a flag is use to control batch normalisation behaviour during inferene on test data\n",
    "    # iteration is the iteration counter used inside the training loop; required for batch normalisation\n",
    "    \n",
    "    num_layers = len(h)\n",
    "    \n",
    "    bn_moving_avg_updates = []\n",
    "\n",
    "    with tf.variable_scope('func_learner'):\n",
    "        # 0th hidden layer\n",
    "        hidden_layer, update_moving_avg = get_layer_complete(x,m,h[0],'hidden_layer_0',is_test,prob_keep,global_step)\n",
    "        bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Other hidden layers\n",
    "        for i in range(num_layers-1):\n",
    "            hidden_layer, update_moving_avg = get_layer_complete(hidden_layer,h[i],h[i+1],'hidden_layer_'+str(i+1),is_test,prob_keep,global_step)\n",
    "            bn_moving_avg_updates.append(update_moving_avg)\n",
    "\n",
    "        # Output layer\n",
    "        weights1= tf.get_variable(name='output_layer_weights',\n",
    "                              shape=[h[-1],n],\n",
    "                              initializer=tf.random_normal_initializer(),\n",
    "                              dtype=DTYPE)\n",
    "\n",
    "    return tf.matmul(hidden_layer,weights1), bn_moving_avg_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM_complete_mccv(input_size=1,output_size=1,hidden_sizes=[5,5],x_data=None,y_data=None,\n",
    "                  batch_size=32, epochs =5, test_fraction=0.1, test_interval=250, initial_learning_rate = 0.02,\n",
    "                  epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 10,\n",
    "                  sgd_switch_epoch = 3,\n",
    "                  prob_keep = 0.8,\n",
    "                  fname_model_out='/tmp/model.ckpt'):\n",
    "    # Using montecarlo crossvalidation\n",
    "    config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 1}\n",
    "    )\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        # Generate test data initially for convenience\n",
    "        test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(x_data,y_data,test_fraction,batch_size)\n",
    "        n_samples = len(y_data)\n",
    "        max_steps = batches.shape[1]\n",
    "            \n",
    "        with tf.name_scope('input'):\n",
    "            # Placeholder variables which will be fed data at train time\n",
    "            x  = tf.placeholder(DTYPE,[None,input_size],name='x_input')\n",
    "            y  = tf.placeholder(DTYPE,[None,output_size],name='y_input')\n",
    "        \n",
    "        with tf.name_scope('control_inputs'):\n",
    "            global_step = tf.Variable(0, trainable=False,name='global_step')\n",
    "            epoch_ind = tf.Variable(0,trainable=False,name='epoch_ind')\n",
    "            increment_epoch_ind = tf.assign_add(epoch_ind,1,name='increment_epoch_ind')\n",
    "            is_test = tf.contrib.eager.Variable(False, trainable=False,name='is_test')\n",
    "\n",
    "        # Predictions of the NN\n",
    "        y_pred, bn_moving_avg_updates = func_deep_learner_arbshape(x,input_size,output_size,hidden_sizes,is_test,global_step,1)\n",
    "\n",
    "        with tf.name_scope('mean_squared_error'):\n",
    "            mse = tf.losses.mean_squared_error(y,y_pred)\n",
    "        tf.summary.scalar('mean_squared_error',mse)\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            global_step_in_epoch = global_step - epoch_ind*max_steps\n",
    "            #learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step_in_epoch, decay_steps, decay_rate, staircase=True)\n",
    "            learning_rate = tf.train.exponential_decay(initial_learning_rate, epoch_ind, epoch_learning_rate_interval, epoch_learning_rate_decay, staircase=True)\n",
    "            tf.summary.scalar('learning_rate', learning_rate)\n",
    "            train_step      = tf.train.AdamOptimizer(learning_rate).minimize(mse, global_step = global_step)\n",
    "            train_step_sgd = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(mse, global_step = global_step)\n",
    "\n",
    "        # Create merged summary object and file writers\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test', sess.graph)\n",
    "        \n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "        # Initialise variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train the model\n",
    "        start_time = time.time()\n",
    "        old_time = start_time\n",
    "        for j in range(epochs):\n",
    "            # At each epoch, regenerate test data, train data, and batches\n",
    "            test_x,test_y,train_x_full,train_y_full,batches = generate_testtrain(x_data,y_data,test_fraction,batch_size)\n",
    "            n_batches = batches.shape[1]\n",
    "            n_test = len(test_y)\n",
    "            epoch_init_time = time.time()\n",
    "                \n",
    "            print('Starting Epoch %d, with %d batches' % (j,max_steps))\n",
    "            for i in range(n_batches):\n",
    "                if i % test_interval ==0:\n",
    "                    # Check how our model performs against the test data\n",
    "                    [summary, mse_val] = sess.run([merged_summaries,mse], feed_dict={x: test_x, y: test_y, is_test: True})\n",
    "                    test_writer.add_summary(summary,i + j*max_steps)\n",
    "                    curr_time = time.time()\n",
    "                    print('\\tEpoch %d, Step %04d, MSE: %4.3e,  LearnRate: %4.3e, Time taken : %4.3fs' % (sess.run(epoch_ind),i, mse_val, sess.run(learning_rate),curr_time-old_time))\n",
    "                    old_time = curr_time\n",
    "                else:\n",
    "                    # Generate new sample data and train our model\n",
    "                    train_x, train_y = get_xy_by_inds(train_x_full,train_y_full,batches[:,i])\n",
    "                    if (i<sgd_switch_epoch):\n",
    "                        summary, _ = sess.run([merged_summaries, train_step], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    else:\n",
    "                        summary, _ = sess.run([merged_summaries, train_step_sgd], feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    sess.run(bn_moving_avg_updates, feed_dict={x: train_x, y: train_y, is_test: False})\n",
    "                    train_writer.add_summary(summary,i+ j*max_steps)\n",
    "            # End of epoch, calculate avg stats\n",
    "            epoch_end_time = time.time()\n",
    "            sess.run(increment_epoch_ind)\n",
    "            avg_mrad = np.sqrt(mse_val)*1000\n",
    "            print('Epoch %d time: %4.3fs, average error = %4.3fmrad' % (j,epoch_end_time-epoch_init_time,avg_mrad))\n",
    "        train_writer.close()\n",
    "        test_writer.close()     \n",
    "        print('\\nTotal time:\\t %4.3fs' % (time.time()-start_time))\n",
    "        \n",
    "        save_path = saver.save(sess, fname_model_out)\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_ai_kup(acc,phase):\n",
    "    n_samples  = acc.shape[0]\n",
    "    seq_length = acc.shape[1]\n",
    "    \n",
    "    input_size  = seq_length\n",
    "    output_size = 1\n",
    "    \n",
    "    hidden_sizes = [256,128,64,32,16,8]\n",
    "    batch_size  = 512\n",
    "    \n",
    "    mccv_steps   = 3000\n",
    "    test_fraction= 0.1\n",
    "    \n",
    "    train_RBM_complete_mccv(input_size=input_size,output_size=output_size,hidden_sizes=hidden_sizes,initial_learning_rate=0.1,\n",
    "                            x_data=acc,y_data=phase,batch_size=batch_size, epochs=mccv_steps,test_fraction=test_fraction,test_interval=50,\n",
    "                            epoch_learning_rate_decay = 0.5, epoch_learning_rate_interval = 500,sgd_switch_epoch = 2000,\n",
    "                            prob_keep = 0.65,fname_model_out='./model_0.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_kup_phase = np.array(list(dset['phase']))\n",
    "ai_kup_acc = np.array(list(dset['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0, with 160 batches\n",
      "\tEpoch 0, Step 0000, MSE: 1.017e+01,  LearnRate: 1.000e-01, Time taken : 0.802s\n",
      "\tEpoch 0, Step 0050, MSE: 2.669e-02,  LearnRate: 1.000e-01, Time taken : 3.231s\n",
      "\tEpoch 0, Step 0100, MSE: 2.660e-02,  LearnRate: 1.000e-01, Time taken : 1.718s\n",
      "\tEpoch 0, Step 0150, MSE: 2.605e-02,  LearnRate: 1.000e-01, Time taken : 1.888s\n",
      "Epoch 0 time: 7.880s, average error = 161.393mrad\n",
      "Starting Epoch 1, with 160 batches\n",
      "\tEpoch 1, Step 0000, MSE: 2.654e-02,  LearnRate: 1.000e-01, Time taken : 0.765s\n",
      "\tEpoch 1, Step 0050, MSE: 2.587e-02,  LearnRate: 1.000e-01, Time taken : 1.596s\n",
      "\tEpoch 1, Step 0100, MSE: 2.597e-02,  LearnRate: 1.000e-01, Time taken : 1.615s\n",
      "\tEpoch 1, Step 0150, MSE: 2.565e-02,  LearnRate: 1.000e-01, Time taken : 1.606s\n",
      "Epoch 1 time: 5.345s, average error = 160.144mrad\n",
      "Starting Epoch 2, with 160 batches\n",
      "\tEpoch 2, Step 0000, MSE: 2.509e-02,  LearnRate: 1.000e-01, Time taken : 0.567s\n",
      "\tEpoch 2, Step 0050, MSE: 2.506e-02,  LearnRate: 1.000e-01, Time taken : 1.524s\n",
      "\tEpoch 2, Step 0100, MSE: 2.511e-02,  LearnRate: 1.000e-01, Time taken : 1.543s\n",
      "\tEpoch 2, Step 0150, MSE: 2.528e-02,  LearnRate: 1.000e-01, Time taken : 1.702s\n",
      "Epoch 2 time: 5.259s, average error = 158.989mrad\n",
      "Starting Epoch 3, with 160 batches\n",
      "\tEpoch 3, Step 0000, MSE: 2.615e-02,  LearnRate: 1.000e-01, Time taken : 0.615s\n",
      "\tEpoch 3, Step 0050, MSE: 2.559e-02,  LearnRate: 1.000e-01, Time taken : 1.741s\n",
      "\tEpoch 3, Step 0100, MSE: 2.548e-02,  LearnRate: 1.000e-01, Time taken : 1.759s\n",
      "\tEpoch 3, Step 0150, MSE: 2.550e-02,  LearnRate: 1.000e-01, Time taken : 1.704s\n",
      "Epoch 3 time: 5.723s, average error = 159.682mrad\n",
      "Starting Epoch 4, with 160 batches\n",
      "\tEpoch 4, Step 0000, MSE: 2.565e-02,  LearnRate: 1.000e-01, Time taken : 0.679s\n",
      "\tEpoch 4, Step 0050, MSE: 2.580e-02,  LearnRate: 1.000e-01, Time taken : 1.469s\n",
      "\tEpoch 4, Step 0100, MSE: 2.574e-02,  LearnRate: 1.000e-01, Time taken : 1.598s\n",
      "\tEpoch 4, Step 0150, MSE: 2.587e-02,  LearnRate: 1.000e-01, Time taken : 1.619s\n",
      "Epoch 4 time: 5.199s, average error = 160.827mrad\n",
      "Starting Epoch 5, with 160 batches\n",
      "\tEpoch 5, Step 0000, MSE: 2.563e-02,  LearnRate: 1.000e-01, Time taken : 0.635s\n",
      "\tEpoch 5, Step 0050, MSE: 2.554e-02,  LearnRate: 1.000e-01, Time taken : 1.567s\n",
      "\tEpoch 5, Step 0100, MSE: 2.551e-02,  LearnRate: 1.000e-01, Time taken : 1.598s\n",
      "\tEpoch 5, Step 0150, MSE: 2.546e-02,  LearnRate: 1.000e-01, Time taken : 1.830s\n",
      "Epoch 5 time: 5.605s, average error = 159.575mrad\n",
      "Starting Epoch 6, with 160 batches\n",
      "\tEpoch 6, Step 0000, MSE: 2.541e-02,  LearnRate: 1.000e-01, Time taken : 0.805s\n",
      "\tEpoch 6, Step 0050, MSE: 2.596e-02,  LearnRate: 1.000e-01, Time taken : 1.806s\n",
      "\tEpoch 6, Step 0100, MSE: 2.432e-02,  LearnRate: 1.000e-01, Time taken : 1.601s\n",
      "\tEpoch 6, Step 0150, MSE: 1.713e-02,  LearnRate: 1.000e-01, Time taken : 1.643s\n",
      "Epoch 6 time: 5.609s, average error = 130.888mrad\n",
      "Starting Epoch 7, with 160 batches\n",
      "\tEpoch 7, Step 0000, MSE: 1.546e-02,  LearnRate: 1.000e-01, Time taken : 0.678s\n",
      "\tEpoch 7, Step 0050, MSE: 1.246e-02,  LearnRate: 1.000e-01, Time taken : 1.810s\n",
      "\tEpoch 7, Step 0100, MSE: 1.022e-02,  LearnRate: 1.000e-01, Time taken : 1.676s\n",
      "\tEpoch 7, Step 0150, MSE: 9.149e-03,  LearnRate: 1.000e-01, Time taken : 1.536s\n",
      "Epoch 7 time: 5.599s, average error = 95.650mrad\n",
      "Starting Epoch 8, with 160 batches\n",
      "\tEpoch 8, Step 0000, MSE: 1.006e-02,  LearnRate: 1.000e-01, Time taken : 0.678s\n",
      "\tEpoch 8, Step 0050, MSE: 9.386e-03,  LearnRate: 1.000e-01, Time taken : 1.615s\n",
      "\tEpoch 8, Step 0100, MSE: 9.556e-03,  LearnRate: 1.000e-01, Time taken : 1.645s\n",
      "\tEpoch 8, Step 0150, MSE: 9.717e-03,  LearnRate: 1.000e-01, Time taken : 1.743s\n",
      "Epoch 8 time: 5.555s, average error = 98.577mrad\n",
      "Starting Epoch 9, with 160 batches\n",
      "\tEpoch 9, Step 0000, MSE: 9.663e-03,  LearnRate: 1.000e-01, Time taken : 0.580s\n",
      "\tEpoch 9, Step 0050, MSE: 9.389e-03,  LearnRate: 1.000e-01, Time taken : 1.834s\n",
      "\tEpoch 9, Step 0100, MSE: 9.216e-03,  LearnRate: 1.000e-01, Time taken : 1.748s\n",
      "\tEpoch 9, Step 0150, MSE: 9.062e-03,  LearnRate: 1.000e-01, Time taken : 1.690s\n",
      "Epoch 9 time: 5.774s, average error = 95.197mrad\n",
      "Starting Epoch 10, with 160 batches\n",
      "\tEpoch 10, Step 0000, MSE: 9.201e-03,  LearnRate: 1.000e-01, Time taken : 0.563s\n",
      "\tEpoch 10, Step 0050, MSE: 9.452e-03,  LearnRate: 1.000e-01, Time taken : 1.626s\n",
      "\tEpoch 10, Step 0100, MSE: 9.443e-03,  LearnRate: 1.000e-01, Time taken : 1.601s\n",
      "\tEpoch 10, Step 0150, MSE: 9.267e-03,  LearnRate: 1.000e-01, Time taken : 1.679s\n",
      "Epoch 10 time: 5.413s, average error = 96.263mrad\n",
      "Starting Epoch 11, with 160 batches\n",
      "\tEpoch 11, Step 0000, MSE: 9.651e-03,  LearnRate: 1.000e-01, Time taken : 0.721s\n",
      "\tEpoch 11, Step 0050, MSE: 9.599e-03,  LearnRate: 1.000e-01, Time taken : 1.612s\n",
      "\tEpoch 11, Step 0100, MSE: 9.290e-03,  LearnRate: 1.000e-01, Time taken : 1.678s\n",
      "\tEpoch 11, Step 0150, MSE: 9.480e-03,  LearnRate: 1.000e-01, Time taken : 1.631s\n",
      "Epoch 11 time: 5.488s, average error = 97.365mrad\n",
      "Starting Epoch 12, with 160 batches\n",
      "\tEpoch 12, Step 0000, MSE: 9.193e-03,  LearnRate: 1.000e-01, Time taken : 0.578s\n",
      "\tEpoch 12, Step 0050, MSE: 9.925e-03,  LearnRate: 1.000e-01, Time taken : 1.805s\n",
      "\tEpoch 12, Step 0100, MSE: 9.193e-03,  LearnRate: 1.000e-01, Time taken : 1.641s\n",
      "\tEpoch 12, Step 0150, MSE: 9.128e-03,  LearnRate: 1.000e-01, Time taken : 1.590s\n",
      "Epoch 12 time: 5.504s, average error = 95.541mrad\n",
      "Starting Epoch 13, with 160 batches\n",
      "\tEpoch 13, Step 0000, MSE: 9.300e-03,  LearnRate: 1.000e-01, Time taken : 0.598s\n",
      "\tEpoch 13, Step 0050, MSE: 9.560e-03,  LearnRate: 1.000e-01, Time taken : 1.706s\n",
      "\tEpoch 13, Step 0100, MSE: 9.627e-03,  LearnRate: 1.000e-01, Time taken : 1.721s\n",
      "\tEpoch 13, Step 0150, MSE: 9.255e-03,  LearnRate: 1.000e-01, Time taken : 1.897s\n",
      "Epoch 13 time: 5.900s, average error = 96.202mrad\n",
      "Starting Epoch 14, with 160 batches\n",
      "\tEpoch 14, Step 0000, MSE: 9.060e-03,  LearnRate: 1.000e-01, Time taken : 0.638s\n",
      "\tEpoch 14, Step 0050, MSE: 9.029e-03,  LearnRate: 1.000e-01, Time taken : 1.746s\n",
      "\tEpoch 14, Step 0100, MSE: 9.075e-03,  LearnRate: 1.000e-01, Time taken : 1.837s\n",
      "\tEpoch 14, Step 0150, MSE: 9.614e-03,  LearnRate: 1.000e-01, Time taken : 1.782s\n",
      "Epoch 14 time: 5.907s, average error = 98.050mrad\n",
      "Starting Epoch 15, with 160 batches\n",
      "\tEpoch 15, Step 0000, MSE: 9.230e-03,  LearnRate: 1.000e-01, Time taken : 0.663s\n",
      "\tEpoch 15, Step 0050, MSE: 9.376e-03,  LearnRate: 1.000e-01, Time taken : 1.591s\n",
      "\tEpoch 15, Step 0100, MSE: 9.029e-03,  LearnRate: 1.000e-01, Time taken : 1.717s\n",
      "\tEpoch 15, Step 0150, MSE: 9.478e-03,  LearnRate: 1.000e-01, Time taken : 1.658s\n",
      "Epoch 15 time: 5.545s, average error = 97.353mrad\n",
      "Starting Epoch 16, with 160 batches\n",
      "\tEpoch 16, Step 0000, MSE: 9.158e-03,  LearnRate: 1.000e-01, Time taken : 0.734s\n",
      "\tEpoch 16, Step 0050, MSE: 9.484e-03,  LearnRate: 1.000e-01, Time taken : 1.712s\n",
      "\tEpoch 16, Step 0100, MSE: 8.934e-03,  LearnRate: 1.000e-01, Time taken : 1.653s\n",
      "\tEpoch 16, Step 0150, MSE: 8.833e-03,  LearnRate: 1.000e-01, Time taken : 1.759s\n",
      "Epoch 16 time: 5.651s, average error = 93.986mrad\n",
      "Starting Epoch 17, with 160 batches\n",
      "\tEpoch 17, Step 0000, MSE: 8.868e-03,  LearnRate: 1.000e-01, Time taken : 0.592s\n",
      "\tEpoch 17, Step 0050, MSE: 9.093e-03,  LearnRate: 1.000e-01, Time taken : 1.706s\n",
      "\tEpoch 17, Step 0100, MSE: 9.397e-03,  LearnRate: 1.000e-01, Time taken : 1.545s\n",
      "\tEpoch 17, Step 0150, MSE: 9.045e-03,  LearnRate: 1.000e-01, Time taken : 1.554s\n",
      "Epoch 17 time: 5.268s, average error = 95.107mrad\n",
      "Starting Epoch 18, with 160 batches\n",
      "\tEpoch 18, Step 0000, MSE: 8.852e-03,  LearnRate: 1.000e-01, Time taken : 0.632s\n",
      "\tEpoch 18, Step 0050, MSE: 9.283e-03,  LearnRate: 1.000e-01, Time taken : 1.601s\n",
      "\tEpoch 18, Step 0100, MSE: 8.907e-03,  LearnRate: 1.000e-01, Time taken : 1.611s\n",
      "\tEpoch 18, Step 0150, MSE: 8.870e-03,  LearnRate: 1.000e-01, Time taken : 1.728s\n",
      "Epoch 18 time: 5.456s, average error = 94.181mrad\n",
      "Starting Epoch 19, with 160 batches\n",
      "\tEpoch 19, Step 0000, MSE: 9.508e-03,  LearnRate: 1.000e-01, Time taken : 0.671s\n",
      "\tEpoch 19, Step 0050, MSE: 9.367e-03,  LearnRate: 1.000e-01, Time taken : 1.717s\n",
      "\tEpoch 19, Step 0100, MSE: 9.349e-03,  LearnRate: 1.000e-01, Time taken : 1.730s\n",
      "\tEpoch 19, Step 0150, MSE: 9.376e-03,  LearnRate: 1.000e-01, Time taken : 1.591s\n",
      "Epoch 19 time: 5.582s, average error = 96.830mrad\n",
      "Starting Epoch 20, with 160 batches\n",
      "\tEpoch 20, Step 0000, MSE: 9.305e-03,  LearnRate: 1.000e-01, Time taken : 0.631s\n",
      "\tEpoch 20, Step 0050, MSE: 9.247e-03,  LearnRate: 1.000e-01, Time taken : 1.832s\n",
      "\tEpoch 20, Step 0100, MSE: 9.656e-03,  LearnRate: 1.000e-01, Time taken : 1.741s\n",
      "\tEpoch 20, Step 0150, MSE: 9.238e-03,  LearnRate: 1.000e-01, Time taken : 1.662s\n",
      "Epoch 20 time: 5.713s, average error = 96.116mrad\n",
      "Starting Epoch 21, with 160 batches\n",
      "\tEpoch 21, Step 0000, MSE: 9.086e-03,  LearnRate: 1.000e-01, Time taken : 0.548s\n",
      "\tEpoch 21, Step 0050, MSE: 8.969e-03,  LearnRate: 1.000e-01, Time taken : 1.640s\n",
      "\tEpoch 21, Step 0100, MSE: 8.961e-03,  LearnRate: 1.000e-01, Time taken : 1.684s\n",
      "\tEpoch 21, Step 0150, MSE: 9.180e-03,  LearnRate: 1.000e-01, Time taken : 1.788s\n",
      "Epoch 21 time: 5.675s, average error = 95.814mrad\n",
      "Starting Epoch 22, with 160 batches\n",
      "\tEpoch 22, Step 0000, MSE: 8.972e-03,  LearnRate: 1.000e-01, Time taken : 0.716s\n",
      "\tEpoch 22, Step 0050, MSE: 8.983e-03,  LearnRate: 1.000e-01, Time taken : 1.633s\n",
      "\tEpoch 22, Step 0100, MSE: 8.974e-03,  LearnRate: 1.000e-01, Time taken : 1.661s\n",
      "\tEpoch 22, Step 0150, MSE: 8.888e-03,  LearnRate: 1.000e-01, Time taken : 1.799s\n",
      "Epoch 22 time: 5.618s, average error = 94.277mrad\n",
      "Starting Epoch 23, with 160 batches\n",
      "\tEpoch 23, Step 0000, MSE: 8.894e-03,  LearnRate: 1.000e-01, Time taken : 0.624s\n",
      "\tEpoch 23, Step 0050, MSE: 8.788e-03,  LearnRate: 1.000e-01, Time taken : 1.659s\n",
      "\tEpoch 23, Step 0100, MSE: 8.840e-03,  LearnRate: 1.000e-01, Time taken : 1.604s\n",
      "\tEpoch 23, Step 0150, MSE: 9.173e-03,  LearnRate: 1.000e-01, Time taken : 1.648s\n",
      "Epoch 23 time: 5.427s, average error = 95.774mrad\n",
      "Starting Epoch 24, with 160 batches\n",
      "\tEpoch 24, Step 0000, MSE: 9.350e-03,  LearnRate: 1.000e-01, Time taken : 0.695s\n",
      "\tEpoch 24, Step 0050, MSE: 9.260e-03,  LearnRate: 1.000e-01, Time taken : 1.629s\n",
      "\tEpoch 24, Step 0100, MSE: 9.125e-03,  LearnRate: 1.000e-01, Time taken : 1.708s\n",
      "\tEpoch 24, Step 0150, MSE: 9.273e-03,  LearnRate: 1.000e-01, Time taken : 1.611s\n",
      "Epoch 24 time: 5.570s, average error = 96.294mrad\n",
      "Starting Epoch 25, with 160 batches\n",
      "\tEpoch 25, Step 0000, MSE: 8.736e-03,  LearnRate: 1.000e-01, Time taken : 0.735s\n",
      "\tEpoch 25, Step 0050, MSE: 8.526e-03,  LearnRate: 1.000e-01, Time taken : 1.697s\n",
      "\tEpoch 25, Step 0100, MSE: 8.425e-03,  LearnRate: 1.000e-01, Time taken : 1.718s\n",
      "\tEpoch 25, Step 0150, MSE: 8.530e-03,  LearnRate: 1.000e-01, Time taken : 1.664s\n",
      "Epoch 25 time: 5.629s, average error = 92.358mrad\n",
      "Starting Epoch 26, with 160 batches\n",
      "\tEpoch 26, Step 0000, MSE: 8.636e-03,  LearnRate: 1.000e-01, Time taken : 0.575s\n",
      "\tEpoch 26, Step 0050, MSE: 8.904e-03,  LearnRate: 1.000e-01, Time taken : 1.624s\n",
      "\tEpoch 26, Step 0100, MSE: 9.078e-03,  LearnRate: 1.000e-01, Time taken : 1.751s\n",
      "\tEpoch 26, Step 0150, MSE: 8.879e-03,  LearnRate: 1.000e-01, Time taken : 1.707s\n",
      "Epoch 26 time: 5.663s, average error = 94.227mrad\n",
      "Starting Epoch 27, with 160 batches\n",
      "\tEpoch 27, Step 0000, MSE: 8.830e-03,  LearnRate: 1.000e-01, Time taken : 0.716s\n",
      "\tEpoch 27, Step 0050, MSE: 9.000e-03,  LearnRate: 1.000e-01, Time taken : 1.784s\n",
      "\tEpoch 27, Step 0100, MSE: 8.911e-03,  LearnRate: 1.000e-01, Time taken : 1.715s\n",
      "\tEpoch 27, Step 0150, MSE: 8.928e-03,  LearnRate: 1.000e-01, Time taken : 1.747s\n",
      "Epoch 27 time: 5.784s, average error = 94.489mrad\n",
      "Starting Epoch 28, with 160 batches\n",
      "\tEpoch 28, Step 0000, MSE: 8.752e-03,  LearnRate: 1.000e-01, Time taken : 0.620s\n",
      "\tEpoch 28, Step 0050, MSE: 8.967e-03,  LearnRate: 1.000e-01, Time taken : 1.597s\n",
      "\tEpoch 28, Step 0100, MSE: 8.638e-03,  LearnRate: 1.000e-01, Time taken : 1.610s\n",
      "\tEpoch 28, Step 0150, MSE: 9.051e-03,  LearnRate: 1.000e-01, Time taken : 1.668s\n",
      "Epoch 28 time: 5.377s, average error = 95.135mrad\n",
      "Starting Epoch 29, with 160 batches\n",
      "\tEpoch 29, Step 0000, MSE: 9.130e-03,  LearnRate: 1.000e-01, Time taken : 0.540s\n",
      "\tEpoch 29, Step 0050, MSE: 8.474e-03,  LearnRate: 1.000e-01, Time taken : 1.849s\n",
      "\tEpoch 29, Step 0100, MSE: 8.845e-03,  LearnRate: 1.000e-01, Time taken : 1.606s\n",
      "\tEpoch 29, Step 0150, MSE: 8.755e-03,  LearnRate: 1.000e-01, Time taken : 1.857s\n",
      "Epoch 29 time: 5.804s, average error = 93.568mrad\n",
      "Starting Epoch 30, with 160 batches\n",
      "\tEpoch 30, Step 0000, MSE: 8.668e-03,  LearnRate: 1.000e-01, Time taken : 0.641s\n",
      "\tEpoch 30, Step 0050, MSE: 8.583e-03,  LearnRate: 1.000e-01, Time taken : 1.667s\n",
      "\tEpoch 30, Step 0100, MSE: 8.933e-03,  LearnRate: 1.000e-01, Time taken : 1.830s\n",
      "\tEpoch 30, Step 0150, MSE: 8.862e-03,  LearnRate: 1.000e-01, Time taken : 1.847s\n",
      "Epoch 30 time: 5.957s, average error = 94.140mrad\n",
      "Starting Epoch 31, with 160 batches\n",
      "\tEpoch 31, Step 0000, MSE: 9.042e-03,  LearnRate: 1.000e-01, Time taken : 0.625s\n",
      "\tEpoch 31, Step 0050, MSE: 9.344e-03,  LearnRate: 1.000e-01, Time taken : 1.702s\n",
      "\tEpoch 31, Step 0100, MSE: 8.930e-03,  LearnRate: 1.000e-01, Time taken : 1.750s\n",
      "\tEpoch 31, Step 0150, MSE: 9.292e-03,  LearnRate: 1.000e-01, Time taken : 1.702s\n",
      "Epoch 31 time: 5.672s, average error = 96.396mrad\n",
      "Starting Epoch 32, with 160 batches\n",
      "\tEpoch 32, Step 0000, MSE: 9.297e-03,  LearnRate: 1.000e-01, Time taken : 0.693s\n",
      "\tEpoch 32, Step 0050, MSE: 9.233e-03,  LearnRate: 1.000e-01, Time taken : 1.629s\n",
      "\tEpoch 32, Step 0100, MSE: 9.051e-03,  LearnRate: 1.000e-01, Time taken : 1.749s\n",
      "\tEpoch 32, Step 0150, MSE: 9.173e-03,  LearnRate: 1.000e-01, Time taken : 1.602s\n",
      "Epoch 32 time: 5.494s, average error = 95.777mrad\n",
      "Starting Epoch 33, with 160 batches\n",
      "\tEpoch 33, Step 0000, MSE: 8.807e-03,  LearnRate: 1.000e-01, Time taken : 0.568s\n",
      "\tEpoch 33, Step 0050, MSE: 8.960e-03,  LearnRate: 1.000e-01, Time taken : 1.583s\n",
      "\tEpoch 33, Step 0100, MSE: 8.844e-03,  LearnRate: 1.000e-01, Time taken : 1.825s\n",
      "\tEpoch 33, Step 0150, MSE: 9.034e-03,  LearnRate: 1.000e-01, Time taken : 1.787s\n",
      "Epoch 33 time: 5.714s, average error = 95.045mrad\n",
      "Starting Epoch 34, with 160 batches\n",
      "\tEpoch 34, Step 0000, MSE: 8.919e-03,  LearnRate: 1.000e-01, Time taken : 0.654s\n",
      "\tEpoch 34, Step 0050, MSE: 8.882e-03,  LearnRate: 1.000e-01, Time taken : 1.630s\n",
      "\tEpoch 34, Step 0100, MSE: 9.246e-03,  LearnRate: 1.000e-01, Time taken : 1.698s\n",
      "\tEpoch 34, Step 0150, MSE: 8.921e-03,  LearnRate: 1.000e-01, Time taken : 1.619s\n",
      "Epoch 34 time: 5.476s, average error = 94.448mrad\n",
      "Starting Epoch 35, with 160 batches\n",
      "\tEpoch 35, Step 0000, MSE: 9.075e-03,  LearnRate: 1.000e-01, Time taken : 0.539s\n",
      "\tEpoch 35, Step 0050, MSE: 8.954e-03,  LearnRate: 1.000e-01, Time taken : 1.578s\n",
      "\tEpoch 35, Step 0100, MSE: 9.205e-03,  LearnRate: 1.000e-01, Time taken : 1.753s\n",
      "\tEpoch 35, Step 0150, MSE: 9.205e-03,  LearnRate: 1.000e-01, Time taken : 1.668s\n",
      "Epoch 35 time: 5.569s, average error = 95.942mrad\n",
      "Starting Epoch 36, with 160 batches\n",
      "\tEpoch 36, Step 0000, MSE: 8.757e-03,  LearnRate: 1.000e-01, Time taken : 0.776s\n",
      "\tEpoch 36, Step 0050, MSE: 8.965e-03,  LearnRate: 1.000e-01, Time taken : 1.736s\n",
      "\tEpoch 36, Step 0100, MSE: 9.536e-03,  LearnRate: 1.000e-01, Time taken : 1.710s\n",
      "\tEpoch 36, Step 0150, MSE: 8.700e-03,  LearnRate: 1.000e-01, Time taken : 1.575s\n",
      "Epoch 36 time: 5.639s, average error = 93.275mrad\n",
      "Starting Epoch 37, with 160 batches\n",
      "\tEpoch 37, Step 0000, MSE: 8.784e-03,  LearnRate: 1.000e-01, Time taken : 0.725s\n",
      "\tEpoch 37, Step 0050, MSE: 8.962e-03,  LearnRate: 1.000e-01, Time taken : 1.674s\n",
      "\tEpoch 37, Step 0100, MSE: 9.083e-03,  LearnRate: 1.000e-01, Time taken : 1.652s\n",
      "\tEpoch 37, Step 0150, MSE: 9.083e-03,  LearnRate: 1.000e-01, Time taken : 1.744s\n",
      "Epoch 37 time: 5.586s, average error = 95.306mrad\n",
      "Starting Epoch 38, with 160 batches\n",
      "\tEpoch 38, Step 0000, MSE: 8.521e-03,  LearnRate: 1.000e-01, Time taken : 0.617s\n",
      "\tEpoch 38, Step 0050, MSE: 8.645e-03,  LearnRate: 1.000e-01, Time taken : 1.613s\n",
      "\tEpoch 38, Step 0100, MSE: 8.569e-03,  LearnRate: 1.000e-01, Time taken : 1.738s\n",
      "\tEpoch 38, Step 0150, MSE: 8.980e-03,  LearnRate: 1.000e-01, Time taken : 1.589s\n",
      "Epoch 38 time: 5.508s, average error = 94.763mrad\n",
      "Starting Epoch 39, with 160 batches\n",
      "\tEpoch 39, Step 0000, MSE: 9.854e-03,  LearnRate: 1.000e-01, Time taken : 0.652s\n",
      "\tEpoch 39, Step 0050, MSE: 9.839e-03,  LearnRate: 1.000e-01, Time taken : 1.713s\n",
      "\tEpoch 39, Step 0100, MSE: 9.247e-03,  LearnRate: 1.000e-01, Time taken : 1.745s\n",
      "\tEpoch 39, Step 0150, MSE: 9.310e-03,  LearnRate: 1.000e-01, Time taken : 1.718s\n",
      "Epoch 39 time: 5.689s, average error = 96.488mrad\n",
      "Starting Epoch 40, with 160 batches\n",
      "\tEpoch 40, Step 0000, MSE: 9.150e-03,  LearnRate: 1.000e-01, Time taken : 0.573s\n",
      "\tEpoch 40, Step 0050, MSE: 8.952e-03,  LearnRate: 1.000e-01, Time taken : 1.583s\n",
      "\tEpoch 40, Step 0100, MSE: 9.166e-03,  LearnRate: 1.000e-01, Time taken : 1.675s\n",
      "\tEpoch 40, Step 0150, MSE: 9.378e-03,  LearnRate: 1.000e-01, Time taken : 1.650s\n",
      "Epoch 40 time: 5.426s, average error = 96.841mrad\n",
      "Starting Epoch 41, with 160 batches\n",
      "\tEpoch 41, Step 0000, MSE: 8.376e-03,  LearnRate: 1.000e-01, Time taken : 0.636s\n",
      "\tEpoch 41, Step 0050, MSE: 8.395e-03,  LearnRate: 1.000e-01, Time taken : 1.649s\n",
      "\tEpoch 41, Step 0100, MSE: 8.300e-03,  LearnRate: 1.000e-01, Time taken : 1.607s\n",
      "\tEpoch 41, Step 0150, MSE: 8.396e-03,  LearnRate: 1.000e-01, Time taken : 1.710s\n",
      "Epoch 41 time: 5.482s, average error = 91.630mrad\n",
      "Starting Epoch 42, with 160 batches\n",
      "\tEpoch 42, Step 0000, MSE: 8.944e-03,  LearnRate: 1.000e-01, Time taken : 0.636s\n",
      "\tEpoch 42, Step 0050, MSE: 9.171e-03,  LearnRate: 1.000e-01, Time taken : 1.733s\n",
      "\tEpoch 42, Step 0100, MSE: 9.045e-03,  LearnRate: 1.000e-01, Time taken : 1.632s\n",
      "\tEpoch 42, Step 0150, MSE: 9.065e-03,  LearnRate: 1.000e-01, Time taken : 1.672s\n",
      "Epoch 42 time: 5.612s, average error = 95.208mrad\n",
      "Starting Epoch 43, with 160 batches\n",
      "\tEpoch 43, Step 0000, MSE: 8.638e-03,  LearnRate: 1.000e-01, Time taken : 0.742s\n",
      "\tEpoch 43, Step 0050, MSE: 8.331e-03,  LearnRate: 1.000e-01, Time taken : 1.678s\n",
      "\tEpoch 43, Step 0100, MSE: 8.441e-03,  LearnRate: 1.000e-01, Time taken : 1.539s\n",
      "\tEpoch 43, Step 0150, MSE: 8.629e-03,  LearnRate: 1.000e-01, Time taken : 1.452s\n",
      "Epoch 43 time: 5.318s, average error = 92.895mrad\n",
      "Starting Epoch 44, with 160 batches\n",
      "\tEpoch 44, Step 0000, MSE: 9.391e-03,  LearnRate: 1.000e-01, Time taken : 0.656s\n",
      "\tEpoch 44, Step 0050, MSE: 8.659e-03,  LearnRate: 1.000e-01, Time taken : 1.726s\n",
      "\tEpoch 44, Step 0100, MSE: 8.808e-03,  LearnRate: 1.000e-01, Time taken : 1.652s\n",
      "\tEpoch 44, Step 0150, MSE: 9.035e-03,  LearnRate: 1.000e-01, Time taken : 1.640s\n",
      "Epoch 44 time: 5.506s, average error = 95.054mrad\n",
      "Starting Epoch 45, with 160 batches\n",
      "\tEpoch 45, Step 0000, MSE: 9.467e-03,  LearnRate: 1.000e-01, Time taken : 0.614s\n",
      "\tEpoch 45, Step 0050, MSE: 8.878e-03,  LearnRate: 1.000e-01, Time taken : 1.490s\n",
      "\tEpoch 45, Step 0100, MSE: 8.921e-03,  LearnRate: 1.000e-01, Time taken : 1.594s\n",
      "\tEpoch 45, Step 0150, MSE: 9.045e-03,  LearnRate: 1.000e-01, Time taken : 1.517s\n",
      "Epoch 45 time: 5.119s, average error = 95.104mrad\n",
      "Starting Epoch 46, with 160 batches\n",
      "\tEpoch 46, Step 0000, MSE: 9.125e-03,  LearnRate: 1.000e-01, Time taken : 0.622s\n",
      "\tEpoch 46, Step 0050, MSE: 8.995e-03,  LearnRate: 1.000e-01, Time taken : 1.704s\n",
      "\tEpoch 46, Step 0100, MSE: 9.421e-03,  LearnRate: 1.000e-01, Time taken : 1.639s\n",
      "\tEpoch 46, Step 0150, MSE: 9.017e-03,  LearnRate: 1.000e-01, Time taken : 1.784s\n",
      "Epoch 46 time: 5.689s, average error = 94.958mrad\n",
      "Starting Epoch 47, with 160 batches\n",
      "\tEpoch 47, Step 0000, MSE: 8.428e-03,  LearnRate: 1.000e-01, Time taken : 0.707s\n",
      "\tEpoch 47, Step 0050, MSE: 8.717e-03,  LearnRate: 1.000e-01, Time taken : 1.726s\n",
      "\tEpoch 47, Step 0100, MSE: 8.873e-03,  LearnRate: 1.000e-01, Time taken : 1.479s\n",
      "\tEpoch 47, Step 0150, MSE: 8.564e-03,  LearnRate: 1.000e-01, Time taken : 1.837s\n",
      "Epoch 47 time: 5.587s, average error = 92.544mrad\n",
      "Starting Epoch 48, with 160 batches\n",
      "\tEpoch 48, Step 0000, MSE: 8.621e-03,  LearnRate: 1.000e-01, Time taken : 0.684s\n",
      "\tEpoch 48, Step 0050, MSE: 8.276e-03,  LearnRate: 1.000e-01, Time taken : 1.559s\n",
      "\tEpoch 48, Step 0100, MSE: 8.369e-03,  LearnRate: 1.000e-01, Time taken : 1.596s\n",
      "\tEpoch 48, Step 0150, MSE: 8.280e-03,  LearnRate: 1.000e-01, Time taken : 1.553s\n",
      "Epoch 48 time: 5.294s, average error = 90.994mrad\n",
      "Starting Epoch 49, with 160 batches\n",
      "\tEpoch 49, Step 0000, MSE: 9.174e-03,  LearnRate: 1.000e-01, Time taken : 0.664s\n",
      "\tEpoch 49, Step 0050, MSE: 8.918e-03,  LearnRate: 1.000e-01, Time taken : 1.861s\n",
      "\tEpoch 49, Step 0100, MSE: 9.286e-03,  LearnRate: 1.000e-01, Time taken : 1.611s\n",
      "\tEpoch 49, Step 0150, MSE: 8.923e-03,  LearnRate: 1.000e-01, Time taken : 1.695s\n",
      "Epoch 49 time: 5.691s, average error = 94.463mrad\n",
      "Starting Epoch 50, with 160 batches\n",
      "\tEpoch 50, Step 0000, MSE: 8.827e-03,  LearnRate: 1.000e-01, Time taken : 0.659s\n",
      "\tEpoch 50, Step 0050, MSE: 8.794e-03,  LearnRate: 1.000e-01, Time taken : 1.562s\n",
      "\tEpoch 50, Step 0100, MSE: 8.773e-03,  LearnRate: 1.000e-01, Time taken : 1.588s\n",
      "\tEpoch 50, Step 0150, MSE: 8.850e-03,  LearnRate: 1.000e-01, Time taken : 1.656s\n",
      "Epoch 50 time: 5.369s, average error = 94.072mrad\n",
      "Starting Epoch 51, with 160 batches\n",
      "\tEpoch 51, Step 0000, MSE: 8.550e-03,  LearnRate: 1.000e-01, Time taken : 0.651s\n",
      "\tEpoch 51, Step 0050, MSE: 8.671e-03,  LearnRate: 1.000e-01, Time taken : 1.519s\n",
      "\tEpoch 51, Step 0100, MSE: 8.869e-03,  LearnRate: 1.000e-01, Time taken : 1.568s\n",
      "\tEpoch 51, Step 0150, MSE: 8.686e-03,  LearnRate: 1.000e-01, Time taken : 1.582s\n",
      "Epoch 51 time: 5.178s, average error = 93.199mrad\n",
      "Starting Epoch 52, with 160 batches\n",
      "\tEpoch 52, Step 0000, MSE: 8.787e-03,  LearnRate: 1.000e-01, Time taken : 0.599s\n",
      "\tEpoch 52, Step 0050, MSE: 9.224e-03,  LearnRate: 1.000e-01, Time taken : 1.573s\n",
      "\tEpoch 52, Step 0100, MSE: 8.679e-03,  LearnRate: 1.000e-01, Time taken : 1.546s\n",
      "\tEpoch 52, Step 0150, MSE: 8.694e-03,  LearnRate: 1.000e-01, Time taken : 1.728s\n",
      "Epoch 52 time: 5.356s, average error = 93.243mrad\n",
      "Starting Epoch 53, with 160 batches\n",
      "\tEpoch 53, Step 0000, MSE: 8.319e-03,  LearnRate: 1.000e-01, Time taken : 0.602s\n",
      "\tEpoch 53, Step 0050, MSE: 8.365e-03,  LearnRate: 1.000e-01, Time taken : 1.616s\n",
      "\tEpoch 53, Step 0100, MSE: 8.557e-03,  LearnRate: 1.000e-01, Time taken : 1.645s\n",
      "\tEpoch 53, Step 0150, MSE: 8.801e-03,  LearnRate: 1.000e-01, Time taken : 1.808s\n",
      "Epoch 53 time: 5.591s, average error = 93.813mrad\n",
      "Starting Epoch 54, with 160 batches\n",
      "\tEpoch 54, Step 0000, MSE: 8.660e-03,  LearnRate: 1.000e-01, Time taken : 0.627s\n",
      "\tEpoch 54, Step 0050, MSE: 8.734e-03,  LearnRate: 1.000e-01, Time taken : 1.757s\n",
      "\tEpoch 54, Step 0100, MSE: 8.989e-03,  LearnRate: 1.000e-01, Time taken : 1.754s\n",
      "\tEpoch 54, Step 0150, MSE: 8.732e-03,  LearnRate: 1.000e-01, Time taken : 1.646s\n",
      "Epoch 54 time: 5.658s, average error = 93.444mrad\n",
      "Starting Epoch 55, with 160 batches\n",
      "\tEpoch 55, Step 0000, MSE: 9.180e-03,  LearnRate: 1.000e-01, Time taken : 0.598s\n",
      "\tEpoch 55, Step 0050, MSE: 8.810e-03,  LearnRate: 1.000e-01, Time taken : 1.666s\n",
      "\tEpoch 55, Step 0100, MSE: 8.908e-03,  LearnRate: 1.000e-01, Time taken : 1.753s\n",
      "\tEpoch 55, Step 0150, MSE: 8.837e-03,  LearnRate: 1.000e-01, Time taken : 1.723s\n",
      "Epoch 55 time: 5.651s, average error = 94.006mrad\n",
      "Starting Epoch 56, with 160 batches\n",
      "\tEpoch 56, Step 0000, MSE: 9.074e-03,  LearnRate: 1.000e-01, Time taken : 0.642s\n",
      "\tEpoch 56, Step 0050, MSE: 8.616e-03,  LearnRate: 1.000e-01, Time taken : 1.685s\n",
      "\tEpoch 56, Step 0100, MSE: 8.523e-03,  LearnRate: 1.000e-01, Time taken : 1.725s\n",
      "\tEpoch 56, Step 0150, MSE: 8.930e-03,  LearnRate: 1.000e-01, Time taken : 1.560s\n",
      "Epoch 56 time: 5.493s, average error = 94.501mrad\n",
      "Starting Epoch 57, with 160 batches\n",
      "\tEpoch 57, Step 0000, MSE: 9.094e-03,  LearnRate: 1.000e-01, Time taken : 0.653s\n",
      "\tEpoch 57, Step 0050, MSE: 8.754e-03,  LearnRate: 1.000e-01, Time taken : 1.708s\n",
      "\tEpoch 57, Step 0100, MSE: 8.699e-03,  LearnRate: 1.000e-01, Time taken : 1.715s\n",
      "\tEpoch 57, Step 0150, MSE: 8.616e-03,  LearnRate: 1.000e-01, Time taken : 1.656s\n",
      "Epoch 57 time: 5.594s, average error = 92.824mrad\n",
      "Starting Epoch 58, with 160 batches\n",
      "\tEpoch 58, Step 0000, MSE: 8.474e-03,  LearnRate: 1.000e-01, Time taken : 0.569s\n",
      "\tEpoch 58, Step 0050, MSE: 8.568e-03,  LearnRate: 1.000e-01, Time taken : 1.604s\n",
      "\tEpoch 58, Step 0100, MSE: 8.873e-03,  LearnRate: 1.000e-01, Time taken : 1.621s\n",
      "\tEpoch 58, Step 0150, MSE: 8.970e-03,  LearnRate: 1.000e-01, Time taken : 1.565s\n",
      "Epoch 58 time: 5.336s, average error = 94.711mrad\n",
      "Starting Epoch 59, with 160 batches\n",
      "\tEpoch 59, Step 0000, MSE: 8.634e-03,  LearnRate: 1.000e-01, Time taken : 0.651s\n",
      "\tEpoch 59, Step 0050, MSE: 8.764e-03,  LearnRate: 1.000e-01, Time taken : 1.620s\n",
      "\tEpoch 59, Step 0100, MSE: 8.823e-03,  LearnRate: 1.000e-01, Time taken : 1.785s\n",
      "\tEpoch 59, Step 0150, MSE: 8.883e-03,  LearnRate: 1.000e-01, Time taken : 1.607s\n",
      "Epoch 59 time: 5.508s, average error = 94.251mrad\n",
      "Starting Epoch 60, with 160 batches\n",
      "\tEpoch 60, Step 0000, MSE: 9.228e-03,  LearnRate: 1.000e-01, Time taken : 0.588s\n",
      "\tEpoch 60, Step 0050, MSE: 8.742e-03,  LearnRate: 1.000e-01, Time taken : 1.575s\n",
      "\tEpoch 60, Step 0100, MSE: 8.881e-03,  LearnRate: 1.000e-01, Time taken : 1.815s\n",
      "\tEpoch 60, Step 0150, MSE: 9.133e-03,  LearnRate: 1.000e-01, Time taken : 1.651s\n",
      "Epoch 60 time: 5.546s, average error = 95.565mrad\n",
      "Starting Epoch 61, with 160 batches\n",
      "\tEpoch 61, Step 0000, MSE: 8.492e-03,  LearnRate: 1.000e-01, Time taken : 0.642s\n",
      "\tEpoch 61, Step 0050, MSE: 8.410e-03,  LearnRate: 1.000e-01, Time taken : 1.698s\n",
      "\tEpoch 61, Step 0100, MSE: 8.531e-03,  LearnRate: 1.000e-01, Time taken : 1.720s\n",
      "\tEpoch 61, Step 0150, MSE: 8.425e-03,  LearnRate: 1.000e-01, Time taken : 1.571s\n",
      "Epoch 61 time: 5.560s, average error = 91.785mrad\n",
      "Starting Epoch 62, with 160 batches\n",
      "\tEpoch 62, Step 0000, MSE: 8.228e-03,  LearnRate: 1.000e-01, Time taken : 0.632s\n",
      "\tEpoch 62, Step 0050, MSE: 8.020e-03,  LearnRate: 1.000e-01, Time taken : 1.871s\n",
      "\tEpoch 62, Step 0100, MSE: 8.980e-03,  LearnRate: 1.000e-01, Time taken : 1.595s\n",
      "\tEpoch 62, Step 0150, MSE: 8.374e-03,  LearnRate: 1.000e-01, Time taken : 1.771s\n",
      "Epoch 62 time: 5.784s, average error = 91.512mrad\n",
      "Starting Epoch 63, with 160 batches\n",
      "\tEpoch 63, Step 0000, MSE: 9.122e-03,  LearnRate: 1.000e-01, Time taken : 0.653s\n",
      "\tEpoch 63, Step 0050, MSE: 9.734e-03,  LearnRate: 1.000e-01, Time taken : 1.792s\n",
      "\tEpoch 63, Step 0100, MSE: 8.950e-03,  LearnRate: 1.000e-01, Time taken : 1.704s\n"
     ]
    }
   ],
   "source": [
    "train_model_ai_kup(ai_kup_acc,ai_kup_phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel_3.6_TF_1.8]",
   "language": "python",
   "name": "conda-env-intel_3.6_TF_1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
